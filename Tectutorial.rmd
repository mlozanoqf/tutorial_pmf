---
title: "Data, pricing, and portfolio analysis in R."
author:
- Dr. Martín Lozano.
- sites.google.com/site/mlozanoqf/
date: "`r format(Sys.time(), '%B %d, %Y. %H:%M:%S')`"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
  word_document: default
  html_notebook: default
  html_document:
    df_print: paged
toc: yes
fontsize: 12pt
subtitle: An introductory approach of quantitative finance.
abstract: This is an introductory applied tutorial suitable for undergraduate students interested in the area of quantitative finance. The main objective is to show how to analyze financial data and extract basic insights from asset pricing and asset allocation applications using the powerful R programming language. We extend examples and codes freely available in Internet, and some of my own. Here, we do not follow a formal approach as most mathematical background is skipped to emphasize the data analysis, model logic, discussion, graphical approach and R coding (literate programming). This is still a work in progress and it is under revision.
---

------------------------------------------------------------------------

# Preface.

We all have different interests and incentives to learn finance. For me, it is a way to better understand the world we are living in. We are surrounded by finance products and services in our daily life. In fact, we are constantly and probably unconsciously assigning resources to achieve objectives, just as we consciously do in finance: make financing decisions to invest in productive projects. As individuals, we face decisions such as rent or buy, take vacations or a summer course, manage our personal budget, asking for a car loan, evaluating a mortgage, saving for our retirement, calculating and paying taxes, etc. Firm managers are interested to gather funds with the lowest possible costs, invest those funds in the most productive assets and employers, sell their products at a competitive price so at the end of the day their revenues exceed their cost to make money. Governments receive funds mainly from taxpayers and they are expected to invest taxpayers money to provide high quality and reliable public services to improve the living standard of people.

According to the OECD, developed and emerging countries and economies have become increasingly concerned about the level of financial literacy (ability to understand and properly apply financial management skills) of their citizens, including young people. This initially stemmed from concern about the potential impact of shrinking public and private welfare systems, shifting demographics, including the ageing of the population in many countries, and the increased sophistication and expansion of financial services. We all face financial decisions and are consumers of financial services in this evolving context. As a result, financial literacy is now globally recognized as an essential life skill.

Let me introduce myself by listing my postgraduate studies and describing my professional background.

My postgraduate studies: I have a Post Doc in Finance from The University of Manchester; a PhD in Quantitative Finance from the University of the Basque Country; a Doctor *Europaeus* mention from several European universities: Manchester Business School; University of Edinburgh; Humboldt University; Aarhus University; University of Vienna; Dublin City University; Queen Mary, University of London; and University of St. Gallen. I have seven postgraduate studies in Statistical Learning, Data Mining, Scientific Analysis of Data, Statistical Methods, Applied Statistics, Finance, and Quantitative Finance. I have a BS in Economics, and other professional certificates mostly in the area of data science.

My professional background: I am a researcher in the area of quantitative finance for the last 10 years. My research has been published in 3 stars journals (high-ranked) according to The Chartered Association of Business Schools[^1], presented in numerous research seminars, and in prestigious international conferences. I have been a lecturer in economics, finance and data science for under and postgraduate levels at different universities in Mexico and the UK for the last 20 years. Also, I have supervised more than 70 dissertations at under and postgraduate academic programs of schools including the London School of Business & Finance; University of London, School of Oriental and African Studies (SOAS); The University of Manchester; Universidad Complutense de Madrid, UDEM, among others.

[^1]: The Chartered Association of Business Schools is considered the voice of the UK's business and management education sector. According to CABS, all 3 stars rated journals publish original and well executed research papers and are highly regarded. These journals typically have good submission rates and are very selective in what they publish. Papers are heavily refereed in 3 stars rated journals.

In case you are interested, my full academic CV is available at sites.google.com/site/mlozanoqf/

# Introduction.

Our approach emphasizes more the intuition of the problems we face in finance, the logic that supports the solutions, and a graphical approach to illustrate concepts and facilitate analysis. We are also interested in explaining concepts and real-life applications by developing examples that start from the data extraction, the problem statement, the proposed solution and the evaluation of the solution.

The main objective of this document is to show how to extract and analyze financial data from basic insights to asset pricing and asset allocation applications using the powerful R programming language. We start by explaining what finance is about and why we require to incorporate a programming language as a way to conduct financial analysis. Any quantitative analysis requires gathering and manipulating data at some initial step, so we continue our journey by showing how to extract a wide variety of financial and economic information making special emphasis on stock market information and in data visualization. Then, we conduct some basic financial analysis based on the firm's financial statements and time series of firm's stock prices. Then, we move from analyzing prices to analyzing asset returns and we introduce the concept of financial risk. With these foundations, then we move into some asset pricing applications to understand what drives asset returns, we discuss the role of risk factors and how asset pricing can help us to make better financing and investment decisions. Given the relevance of future asset prices changes we also incorporate some foundations of asset prices forecast. Finally, we introduce the portfolio analysis by showing how an investor could take informed decisions to optimize the performance of his or her investment portfolio, how to reduce risk by implementing diversification tools and financial algorithms, implement and evaluate investment strategies with the help of R.

## Finance.

In finance we mainly study financing and investment decisions under uncertainty. Financing decisions are about looking for funds whereas investment decisions are about assigning funds to run a project. Thus, financing and investment are basically two sides of the same coin called project, business idea, firm, financial asset, countries, etc. A business project is a series of inputs, outputs, investment plans and tasks that need to be completed in order to reach a specific expected outcome in the future. Projects are uncertain by nature because many things can go wrong in the future: a firm might go bankrupt, a business idea can be stolen, sales projections might not be as good as expected, plans could change because of coronavirus pandemic, and so on. This is why we say that financing and investment decisions are taken under uncertainty. Finance decisions are taken today, but their results are seen in the uncertain future. Risky projects and uncertain projects are not necessarily bad projects as uncertainty and risk boost innovation, demands a high-quality quantitative analysis, represent opportunities for entrepreneurs, and returns for investors.

An individual takes financing decisions in the job market, trying to get the maximum salary in the most convenient job to get the funds to pay for food, housing and leisure. This individual takes an investment decision when she decides to use her savings to start a small firm. If this firm performs well in the first year, then the firm will apply for a bank loan to finance a business expansion. Once the firm gets the new funds, the firm will have to invest this money wisely in productive assets, technology and hire experts in the relevant business field. As the firm generates profits, the owner who initially decided to risk her money will get returns. These returns may attract other investors willing to help the manager to export to other countries in exchange for some participation in the firm's profits. When we replicate this in the economy many times, it is easy to understand how good finance and investment decisions contribute to the economic growth.

Finance can be applied by individuals and firms as illustrated above. Governments get funds from taxpayers to invest in public assets such as education, health, public infrastructure, etc. Governments also can contribute to create certainty about the stability of economic indicators and maintain a rule of law to stimulate private investment. If public spending remains higher than income, the country might fall into a public deficit and this could lead to financial instability which dampers not only public finances but personal and private investment decisions as well.

Most finance decisions are taken based on a risk-return analysis. In particular, if after doing the corresponding maths and consulting with the pillow you conclude that the expected return is higher than the associated risk, then you will most likely go for it. On the other hand, if the risk looks quite high compared with the expected return, you will surely re-think or abandon the project. We, as individuals, perceive the risk and the return differently. Consider the following real example. For each of the past 17 years, the All-England Lawn Tennis Club has paid for an insurance policy to guard against losses if Wimbledon should have to be cancelled in the event of a worldwide pandemic. This was considered for some as an excessive cost until recently. Wimbledon received about £114 million because 2020 tournament was cancelled due to the coronavirus. A similar thing happens when you buy a used car. Surely the buyer considers the car cheap enough, and the seller considers the car price expensive enough to close the deal. So, it is good to have different perspectives about prices, risk and returns as this allows commercial and financial transactions to exist. We also perceive risk and return differently as we use different methods, data and procedures to estimate risk and return.

Although we all perceive risk and return differently by nature, we may also perceive the risk and return wrong by lack of knowledge. This is not uncommon as we may apply political decisions to finance problems, or make financial decisions without the relevant knowledge in the field, or ignoring the power of data analysis. Underestimating risks and overestimating returns may be as harmful as overestimating risk and underestimating returns. The first could lead to an excess of risk and the latter could lead to forego a good business. We are not suggesting everybody should become a finance expert, but finance professionals are expected to contribute to make better and correct financial decisions most of the times.

Finance is not a pure exact area of knowledge, it borrows some principles of physics and mathematics to develop financial models.[^2] For instance, we are sure that at standard atmospheric pressure, water boils at approximately 100 degrees Celsius. But we do not know for sure whether my business profits will grow at 10% next year. Returns are uncertain, this is why we call them expected returns. In fact, after doing some financial analysis, I can estimate that my profits will grow in the range of 5% to 15%, this range shows how uncertain my profits are. This is why returns and risk are two main pillars in finance. The ability of understanding the economic conditions, the market, and the firm will determine the success of financing and investment decisions. This means that finance requires some knowledge of economics, statistics, math, accounting, probability, marketing, psychology, and data science to transform data into intelligent decisions.

[^2]: The Black-Scholes formula for pricing European call and put options is one of the most famous equations in financial mathematics. This equation is so important that Robert Merton and Myron Scholes received the 1997 Nobel Price for Economics in honour of their work. Unfortunately, Fischer Black, who clearly contributed extensive work to the formula, passed away in 1995. Interestingly, the Black-Scholes formula is basically a partial differential equation (PDE) well known in physics as the "heat equation" which describes the distribution of heat in a given region over time. Moreover, there are close parallels between random movements of particles in a fluid (called physical Brownian motion) and price fluctuations in financial markets (known as financial Brownian motion). Thus, finance seems to follow not only human behaviour but also some physics principles.

As an economist, I consider finance an area within economics. The Journal of Economic Literature (JEL) classification system is used to classify articles, dissertations, books, book reviews, and working papers in EconLit, and in many other applications. The JEL classify finance as "financial economics" and includes

-   G00. Financial Crises.
-   G1. General Financial Markets, Portfolio Choice, Investment Decisions, Asset Pricing, Trading Volume, Bond Interest Rates, Contingent Pricing, Futures Pricing, Information and Market Efficiency, Event Studies, Insider Trading, International Financial Markets, Financial Forecasting and Simulation, Government Policy and Regulation.
-   G2. Financial Institutions and Services, Banks, Depository Institutions, Micro Finance Institutions, Mortgages, Insurance, Insurance Companies, Actuarial Studies, Non-bank Financial Institutions, Financial Instruments, Institutional Investors, Investment Banking, Venture Capital, Brokerage, Ratings and Ratings Agencies, Government Policy and Regulation.
-   G3. Corporate Finance and Governance, Capital Budgeting, Fixed Investment and Inventory Studies, Capacity, Financing Policy, Financial Risk and Risk Management, Capital and Ownership Structure, Value of Firms, Goodwill, Bankruptcy, Liquidation, Mergers, Acquisitions, Restructuring, Corporate Governance, Payout Policy, Government Policy and Regulation.
-   G4. Behavioral Finance, Role and Effects of Psychological, Emotional, Social, and Cognitive Factors on Decision Making in Financial Markets.
-   G5. Household Finance, Household Saving, Borrowing, Debt, and Wealth, Insurance, Financial Literacy.

In this tutorial we focus on only two areas of finance: asset pricing and portfolio analysis. Asset pricing allows us to understand the risk-return relationship of financial instruments and portfolio analysis allows us to take optimal investment decisions in the financial markets.

## Computers.

Every area of knowledge requires computers to conduct interesting analysis and applications. Traditionally, people use good commercials (and unfortunately expensive) software such as Microsoft Excel, SPSS, STATA, E-Views, and many others. These commercial software are good. However, you have to be aware that these programs are fully controlled by private firms who genuinely seek to create value for their shareholders, so there is no guarantee that their associated file formats could be readable in the future, or even exist in the future, which negatively impacts reproducibility. I never advise not to learn commercial software like the ones listed above; but I always encourage learning and use R (or Python) for serious data analysis. These computer languages are user-oriented and are created and constantly improved by a growing scientific community with an immense online presence to assist users.

Commercial software products as the ones listed above are definitely important in the job market, but you also have to realize that the main interaction with these programs is by using the mouse to click on pre-defined, limited and inflexible menus. This kind of user-interaction is most of the times ephemeral and unrecorded, so that many of the choices made during a full quantitative procedure are frequently undocumented and this turns out to be highly problematic because there is no trace about how an analysis was conducted, and also because it becomes hard to propose an extension to the analysis in phases or replication in different contexts. Coding allows us to conduct and develop reproducible research. Learning how to code is equivalent as writing a cooking recipe and every time you click run you get the dish done. Although, chefs have to pay for ovens, kitchen items and even ingredients, while in finance most of our inputs are free data and the technology is also free as R is an open source software.

Other commercial products in which you do not have to code like Microsoft Excel, SPSS, STATA, E-Views and many others, have high licensing fees and also rely on mysterious black boxes to produce a battery of results. These black boxes are problematic because the data comes in and the result comes out as magic, showing no details about the procedure followed to produce the final results, and the user could sadly get the wrong illusion that he or she understands data analysis. This might be convenient in some specific and limited cases but in others you miss the fun that represents having access to all the details of the computation and limit the extent to which you can customize or extend to innovative and create new improved applications. The general alternative to using a point-and-click program is to familiarize with languages like R which allows writing scripts to program algorithms for economic and financial analysis and visualizations.

## R and RStudio.

R is a language and environment for statistical computing and graphics. R is a powerful integrated suite of software facilities for data manipulation, calculation and graphical display. R is available as Free Software under the terms of the Free Software Foundation's GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS. Given its popularity and flexibility, R is currently implemented in virtually all areas of knowledge including finance by students, practitioners, researchers, universities, institutions, firms, think tanks, and policy makers around the world.

Many users think of R as a statistics system. We prefer to think of it as an environment within which statistical techniques are implemented. This is why R is a popular choice for finance and economic modeling. R can be extended (easily) via packages as we will show in this tutorial. There are about eight default packages supplied with the R distribution and many more are available through the CRAN family of Internet sites covering a very wide range of modern statistics, data science and finance applications.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# This removes all items in environment. 
# It is a good practice to start your code this way.
rm(list=ls())
library(tictoc)
tic()
```

Let's see how many packages are there as today using R code.

```{r}
# The function is available.packages and we store the result 
# in R_packages variable. 
R_packages <- available.packages(filters = "duplicates",
                                 repos="https://cran.rstudio.com")
# Now, we combine paste and print functions to produce a sentence.
# Note that nrow simply counts the number of rows in R_packages.
print(paste("There are", nrow(R_packages), 
            "R packages available in CRAN as of", 
            Sys.Date()))
```

Every R package has its own PDF online documentation and there are many online examples developed by users as well. My recommendation here in case you have a question about this is Google it. Many times, we do not know how to deal with an error message, and we can find our way out by Google it. I have been using R, Matlab, Octave and other languages for the last 15 years and I can confirm every time I do not know how to code something in R, I can easily find a solution online either in discussion forums, documentation, or in YouTube videos.

R offers numerous advantages for data analysis compared with other alternatives like Microsoft Excel. R is free; it is easy to do reproducible research (self-documenting, repeatable); it is scalable (applicable to small or large problems); there is a big and growing R online community by discipline and by region (including R-Ladies groups); Stack Overflow; plenty of learning resources (quantity and quality); many R books and resources (see the reference list at the end of this tutorial). Finally, R is 'becoming' the new norm in data science and specifically in finance analysis. Even if you are interested in other languages like Python, at the end learning one language can help you to understand others. Microsoft Excel is a great tool and we are expected to learn and use it very well, but it is not the best alternative for data analysis.

While some people find the use of a commandline environment for coding daunting, it is becoming a necessary skill for managers, management analysts, and data scientists as the volume and variety of data has grown. Thus, scripting or programming has become a third language for modern professionals, in addition to their native language, and discipline specific terminology.

There are some videos in the reference list to illustrate the process of downloading and installing R and RStudio in PC and Mac personal computers. However, you know webpages versions are frequently updated and for this reason some steps in old videos might be invalid. My recommendation is to follow these two YouTube videos that show the steps for the latest known versions of R and RStudio as of early July 2020.

-   Download & Install R 3.6.3: <https://youtu.be/3OxPMYP8lNU>
-   Download & Install RStudio Desktop 1.3.959: <https://youtu.be/uvnuQ_fKrMc>
-   I do not use MacOS, but this looks good for Mac users: Installing R and Rstudio on MacOS <https://youtu.be/Y20P3u3c_1c>
-   RStudio: A Guided Tour (by Jamison Crawford) <https://youtu.be/xgPwDlAtuNI>
-   Why Study a Postgraduate Degree in Quantitative Finance (by one of my current MSc student): <https://youtu.be/7xpydgOrzkw>

I am sure there might be other sources or tutorials online to help you out in the process of installing R and RStudio. If you find a good one please share it.

There are many online and free resources to continue learning R and finance applications in R in the future:

-   Swirl. Swirl teaches you R programming and data science interactively, at your own pace, and right in the R console! <https://swirlstats.com/>
-   Learnr. The learnr package makes it easy to turn any R Markdown document into an interactive tutorial. Tutorials consist of content along with interactive components for checking and reinforcing understanding. <https://rstudio.github.io/learnr/>
-   Just Google your questions, you will be amazed about the available online help, resources and user's examples.

First time users of R are often confused as to the difference between R and RStudio. RStudio is actually an add-on to R: it takes the R software and adds to it a very user-friendly graphical interface. In simple terms, consider R as a car's powerful engine, and RStudio as a very modern and convenient car's dashboard. Thus, when we use RStudio, we are still using the full version of R while also getting the benefit of greater functionality and usability due to an improved RStudio user interface. In this tutorial, we are going to use RStudio. In fact, this PDF tutorial has been produced in RStudio (not in MS-Word), using R Markdown package.

Since RStudio is an add-on to R, you must first download and install R as well as RStudio, two steps which are done separately. R and RStudio are not separate versions of the same program, and cannot be substituted for one another. R may be used without RStudio, but RStudio may not be used without R. On your computer, you will see R and RStudio as separate installed programs. When using R for data analysis, you will always open and work in RStudio; you must leave R installed on the computer for RStudio to work, even though you will likely never open R itself.

Below is the way we show R code and R output throughout this tutorial.

```{r}
# We write comments in italics. The R code looks like this:
print("hello world")
```

The output of the R code above is a string text: "hello world". The R function *print* prints in the screen the quoted string in parenthesis. We will use a number of R functions to illustrate finance applications and examples. Nobody expects you to memorize each function and its syntax as you can always access R documentation to help you out with R functions syntax and even examples. This is done by typing *?print* in the console. You will see a right panel with the corresponding help. This is part of the help documentation when you type *?print*:

Print Values. Description. *print* prints its argument and returns it invisibly (via invisible(x)). It is a generic function which means that new printing methods can be easily added for new classes. Usage. print(x, ...)

Now, a simple numerical example to show R code and R output.

```{r}
# Define the value of a.
a = 2 + 2
# Print the value of a.
print(a)
# Or simply
a
```

So, *a* is equal to 4. Of course, R is more than a Fisher-Price calculator. But it is always useful to see simple examples first.

In the following sections, we show and explain examples that the participants can replicate for their own purposes and interests. In particular, we expect the participants to "copy and paste" the structure of the code to replicate some analysis on their own. For example, imagine you are interested in producing a variable *b* that contains the result of $3+3$. Given the previous example, you know you can do this by typing $b = 3 + 3$. A variable *c* which is the product of *a* and *b* is then $c = a \times b$. You will soon realize that there are many ways in which we can define $c$. An alternative is $c = (2+2) * b$, or $c = a * (3+3)$, $c = (2+2) * (3+3)$, and even $c = 4 * 6$. This means that there are many equivalent ways to do one task.

# Financial data.

Data is everywhere and finance has always been about data.[^3] Industries perceive data as an essential commodity and fuel to take decisions under uncertainty. As a matter of fact, data science and finance go hand in hand. Even before the term data science was coined, finance was using it. Data Science is widely used in areas like risk analytics, credit risk, fraud detection, risk management, pricing, and algorithmic trading. Financial institutions were among the earliest users and pioneers of data analytics. We need data to perform financial analysis, estimate financial models, forecast financial variables, take financing and investment decisions, and more. The alternative of data is to listen to your guts, your intuition, or experience. This alternative is not bad by itself, but it would not be our main approach here.

[^3]: According to market intelligence company IDC, the 'Global Datasphere' in 2018 reached 18 zettabytes. The vast majority of the world's data has been created in the last few years and this astonishing growth of data shows no sign of slowing down. In fact, IDC predicts the world's data will grow to 175 zettabytes in 2025. One zettabyte is 1000000000000000000000 bytes. In scientific notation this is 1e+21 or 1 with 21 zeros at the right, we call it one sextillion.

Financial data is usually free and available in electronic sources for downloading. There are exemptions as private firms are not obligated to make their financial information public. Also, there are some kind of financial information that public firms are not obligated to make public given the international regulations. Sometimes, financial data is not fully available but we have summary statistics that can be quite useful to simulate the data by ourselves. Financial markets are a great source of financial information like exchange rates, commodities, financial instruments, etc. Public institutions and international organizations are useful sources of financial information as well. In any case, we have more financial data than time and resources to analyze it, and the gap is getting bigger and bigger.

## Financial statements.

Financial data comes in a wide variety of formats, and is reported and stored in numerous electronic sources. Financial statements represent a great source to analyze and understand the financial health of a firm, individuals and even governments. Financial statements are standard and well-organized reports that summarize important financial accounting information of a business. The traditional approach to analyze financial statements is looking at those in the firm's annual reports, or downloading them in PDF or Excel format in the firm's investor website site.

In R, we can download public firm's financial statements from the US Securities and Exchange Commission using the *finreportr* package (there are others like *finstr*). This is useful to evaluate a firm performance and conduct financial analysis as we do not need to visit the firm site. In fact, we could replicate or extend the full analysis by running the code instead of downloading newer financial statements.

In case you are not familiar with R packages, you should know you need to download the packages you need before using them. This is an easy process in RStudio. First, select the "Tools" menu, and then "Install Packages..." option. Now, type the name of the package you need, in this case type *finreportr* for accessing financial statements (later we will need *tidyquant* as well and others), leave the default options as they are, and click "Install". You will see a typical bar progress of this download process and R will let you know when this is done. You only need to download packages once, but you have to load the package (using the function *library*) every time you will use the package in your script. There are plenty of YouTube tutorials showing this installation process step by step, for example <https://youtu.be/JBcVi-fAT_k>

If you are interested in working with financial statements, you should look for the *finreportr* package documentation online or users' examples available in Internet. Perhaps the main drawback of this package is that it may be slow, you normally need to wait for R to download the financial reports. Let's start with some examples here. First, we load the previously downloaded packages.

```{r}
# Load the finreportr package to download financial reports.
library(finreportr)
# Load the tidyquant package to manipulate the data and more.
library(tidyquant)
library(dplyr)
library(tidyr)
```

Let's analyze Facebook.

```{r}
FB.Info <- CompanyInfo("FB")
print(FB.Info)
```

This is the physical address of the firm. Now some information about Facebook's annual reports.

```{r}
FB.reports <- AnnualReports("FB")
FB.reports$accession.no
```

These are keys to name specific financial reports. You can Google one of those keys above and see the kind of information available at SEC website. Now, let's download a balance sheet.

```{r}
# Store the balance sheet of Facebook (FB) in BS_FB.
BS_FB <- GetBalanceSheet("FB", 2018)
# Show the value in the screen (in paper).
BS_FB
```

We are done. We have the balance sheet information of Facebook in \texttt{BS\_FB} in basically two lines of code without even visiting the firm's website. However, it does not look the same as we are used to seeing in a typical format like PDF or Excel. To be honest, it is difficult to read to say the least. R takes the information from the annual report, so if we ask for 2018, we retrieve information for the previous years.

Let's review how we can manipulate \texttt{BS\_FB} contents to get a more readable result. Balance sheets are annual, so we do not need the *startDate* column, and we know it is expressed in US dollars, so we do not need the *Units* column as well. Imagine we are interested in a clear version of the 2017 balance sheet.

```{r}
# First make a copy.
BS_2017 <- BS_FB %>%
  # convert Amount from character to numeric.
  mutate(Amount = as.numeric(Amount)) %>%
  # Let's extract only 2017.
  filter(endDate == "2017-12-31") %>%
  # Let's show only two columns.
  select(Metric, Amount)
# Show the value in the screen (in paper).
BS_2017
```

This looks more like a familiar balance sheet.

This is the first time we introduce the use of pipes (%\>%) in the code. Pipes are a powerful tool for clearly expressing a sequence of multiple operations mainly in the context of *tidyverse* packages. In the code above we first create a variable for the 2017 balance sheet \texttt{BS\_2017} as the same as the previously created variable \texttt{BS\_FB}, then we transform one column to numeric values so we can make further operations, then we filter the whole information to keep only 2017 values, and finally we select only two columns (the name and value of each account). There might be even more efficient ways to achieve the same result. Note that every sequence is linked with a pipe (%\>%). Pipes process a data-object using a sequence of operations by passing the result of one step as input for the next step.

A balance sheet is a financial statement that reports a company's assets, liabilities and shareholders' equity. The balance sheet is one of the three (income statement and statement of cash flows being the other two) core financial statements used to evaluate a business performance. The balance sheet allows us to verify the so-called accounting equation: $assets = liabilities + equity$. This is equivalent as saying that the firm has what the firm owes.

In plain English the balance sheet shows the value of what the firm has to produce (assets), and where did the money come from (banks in the form of liabilities and/or owners in the form of equity). The balance sheet is used to measure the firm size, the evolution of the firm's debt, and the participation of the owners to finance the firm. It is also useful to measure firm's liquidity and credit risk, among many other things.

Let's extract the 2016 and 2017 balance sheets and show the most relevant rows to verify how the accounting equation holds.

```{r}
# Define the accounts we need in our balance sheet summary.
accounts <- c("Assets" , "Liabilities", 
              "Stockholders' Equity Attributable to Parent", 
              "Liabilities and Equity")
# At the beginning, the summary is a copy of the full statement.
BS_2016_2017_summary <- BS_FB %>%
  # Convert Amount to numeric values (so we can do math operations).
  mutate(Amount = as.numeric(Amount)) %>%
  # This is convenient but not strictly necessary.
  mutate(Metric = factor(Metric, levels = unique(Metric))) %>% 
  # Here we spread rows into columns to have information per year.
  spread(endDate, Amount) %>%
  # Select only three columns.
  select(Metric, "2016-12-31", "2017-12-31") %>%
  # Select only the relevant accounts we are interested in.
  filter(Metric %in% accounts) %>%
  # Net income account is lengthy, let's rename it.
 mutate(Metric = recode_factor(Metric, 
        `Stockholders' Equity Attributable to Parent` = "Equity"))
# Now, show the results.
BS_2016_2017_summary
```

As expected, the accounting equation holds in both 2016 and 2017. We were not expecting a different result. The purpose of this example is to show how we can download and manipulate financial statements of firms. As shown above, the only thing we need is the ticker symbol of the firm and voilà. A ticker symbol or stock symbol is an abbreviation used to uniquely identify publicly traded firm shares. A stock symbol may consist of letters, numbers or a combination of both.

Looking at the USD values is difficult to have a gasp about which balance sheet account increased more or increased less in this period of time. This is, looking at absolute values (USD) makes it difficult to clearly see which account increased more than others, so we normally transform the data to relative terms (ratios, percentage changes, or proportions given a total).

Let's propose a different way to see the balance sheet statement.

```{r}
# First we make a copy.
BS_2016_2017_change <- BS_2016_2017_summary %>%
  # Calculate the percentage change from 2016 to 2017.
  mutate(Year.Change = (`2017-12-31`-`2016-12-31`)/`2016-12-31`)
# Show the results.
BS_2016_2017_change
```

Liabilities increased more than twice compared with assets from 2016 to 2017. Should we be worried about Facebook financial health? One simple way to dig deeper into this issue is to show an alternative balance sheet showing the accounts as relative proportions with respect to the assets.

```{r}
# Create a copy of the summary.
BS_2016_2017_prop <- BS_2016_2017_summary %>%
  # Proportions with respect to the assets (2016).
  mutate(prop2016 = `2016-12-31`/`2016-12-31`[1]) %>%
  # Proportions with respect to the assets (2017).
  mutate(prop2017 = `2017-12-31`/`2017-12-31`[1])
# Show the results.
BS_2016_2017_prop
```

We are now less worried about the financial health of Facebook. Even though the liabilities increased about 76% from 2016 to 2017, these values are still low with respect to the firm's assets. In 2016 their liabilities were about 9% of their assets and 12% in 2017.

Let's plot.

```{r}
accounts_BS <- c("Assets" , "Liabilities", 
              "Stockholders' Equity Attributable to Parent")

BS_2016_2017_plot <- BS_FB %>%
  mutate(Amount = as.numeric(Amount)) %>%
  mutate(Date = as.Date(endDate)) %>%
  mutate(Metric = factor(Metric, levels = unique(Metric))) %>% 
  select(Metric, Amount, Date) %>%
    filter(Date == "2016-12-31" | Date == "2017-12-31") %>%
  filter(Metric %in% accounts_BS) %>%
 mutate(Metric = recode_factor(Metric, 
`Stockholders' Equity Attributable to Parent` = "Equity")) %>%
 mutate(., Metric = 
           factor(Metric, levels = c("Assets", 
                                     "Liabilities", "Equity")))
BS_2016_2017_plot
```

Having the data as in \texttt{BS\_2016\_2017\_plot} makes it easier to use \texttt{ggplot}.

```{r}
library(ggplot2)
g <- ggplot(BS_2016_2017_plot, 
            aes(Metric, Amount, fill = Amount)) +
  ggtitle("Facebook - Balance sheet in USD") +
  geom_bar(stat = "identity") + 
  scale_fill_gradient(low = "red", high = "blue")+
  facet_wrap(~ Date)
g
```

Imagine all balance sheet accounts increased as in the case of Facebook, but the firm is losing money according to the income statement. In this case, we could say there is something wrong with Facebook. On the other hand, if these increases in the balance sheet accounts are supported by an increase in profits for the same period, then things are going well in the firm. Thus, we need Facebook income statement to verify the firm's profits and evaluate whether we should be worried about the increase in the liabilities.

Let's download the income statement and show only the revenues and profit for 2016 and 2017. Note that the process in R is basically the same as in the case of the balance sheet before. This is important as coding allows us to replicate or reproduce the analysis easily. As the process was recorded in the balance sheet, it is only a matter of adapting the same instructions to perform a similar manipulation for the income statement. You might see several lines of code but in reality, it is almost a copy-paste from the previous code. In fact, we are using the comments to emphasize that this code is not new, and is basically a copy of our previous procedure.

```{r}
# Define accounts just as we did before for the balance sheet.
accounts_IS <- c("Revenues" , 
  "Net Income (Loss) Available to Common Stockholders, Basic")
# Instead of GetBalanceSheet, we use GetIncome.
IS_FB <- GetIncome("FB", 2018)
  # Make a copy (as before).
IS_2016_2017_summary <- IS_FB %>%
  # Exactly the same as before.
  mutate(Amount = as.numeric(Amount)) %>%
  # Exactly the same as before.
  mutate(Metric = factor(Metric, levels = unique(Metric))) %>% 
  # Select the required columns.
  select(Metric, Amount, endDate) %>%
  # Exactly the same as before.
  spread(endDate, Amount) %>%
  # Exactly the same as before.
  select(Metric, "2016-12-31", "2017-12-31") %>%
  # Similar as before.
  filter(Metric %in% accounts_IS) %>%
  # Similar as before.
  mutate(Metric = recode(Metric, 
  `Net Income (Loss) Available to Common Stockholders, Basic` 
  = "Net Income"))
# Similar as before.
IS_2016_2017_summary
```

In this code above we explicitly repeat how many times we reproduce the same or very similar code as we did in the case of the balance sheet. This is because it is important to highlight that coding allows us to reproduce our analysis easily. Imagine you are interested to compare balance sheet and income statements for 5 companies using a software that requires using the mouse to click on pre-defined menus. In that case you will have to repeat the procedure step by step and hope to remember so you do not skip anything. In R we code so our procedure is recorded in a series of instructions. Now consider that you need to add two accounts to your analysis. In that case you will only have to change this line:

```{=tex}
\texttt{accounts\_IS <- c("Revenues" , 
"Net Income (Loss) Available to Common Stockholders, Basic")}
```
and add a third or fourth account name and that's it.

Now we have the relevant information we need. The profits increased more than 50% from 2016 to 2017, so apparently the firm knows how to make money. The new asset investments allow the firm to increase the profits. We can calculate a simple financial ratio to find out the relationship between assets (in the balance sheet, representing the firm investment) and profits (in the income statement, shows the ability of the firm to make money). A financial ratio or accounting ratio is a relative magnitude of two selected numerical values taken from an enterprise's financial statements. There are many standard ratios used to try to evaluate the overall financial condition of a corporation.

-   $AP_{2016} = 6.4961/1.0188 = 6.376227$
-   $AP_{2017} = 8.4524/1.5920 = 5.309296$

We normally are interested in making these calculations automatically, without copying the values and taking them directly from the correspondent variables:

```{r}
BS_2016_2017_summary[1,2] / IS_2016_2017_summary[2,2]
BS_2016_2017_summary[1,3] / IS_2016_2017_summary[2,3]
```

$AP_{2016}$ and $AP_{2017}$ represent the ratio of assets and profits for 2016 and 2017. Then, the firm requires 6.38 USD asset investment to make 1 USD in profits in 2016 and only 5.31 USD in 2017. We can also view the same thing the other way around, this is profits over assets. If we do this, then the firm makes 0.1568326 USD for every dollar invested in the asset in 2016 and 0.1883489 USD in 2017. This is clearly good news as the firm's assets are more productive according to this simple analysis.

Let's calculate the revenues with respect to profits.

```{r}
# Make a copy.
IS_2016_2017_prop <- IS_2016_2017_summary %>%
  # Calculate proportion (2016).
  mutate(prop2016 = `2016-12-31`/`2016-12-31`[2]) %>%
  # Calculate proportion (2017).
  mutate(prop2017 = `2017-12-31`/`2017-12-31`[2])
# Show results.
IS_2016_2017_prop
```

According to this ratio, in 2016 the firm transformed 2.7 USD of sales into 1 USD profit, and in 2017 the firm required only 2.5 USD. Accounting ratios like this one can be used for financial managers as corporate targets or firm's objectives. Imagine most of the rest of the firms in the same industry only need 2 USD to generate 1 USD profit. In that case, the financial manager might propose an objective for next year to reduce costs so they can transform 2 USD sales into 1 USD profits for the next year.

Let's plot the income statement basics. First, some necessary changes.

```{r}
IS_2016_2017_plot <- IS_FB %>%
  mutate(Amount = as.numeric(Amount)) %>%
  mutate(Date = as.Date(endDate)) %>%
  mutate(Metric = factor(Metric, levels = unique(Metric))) %>% 
  select(Metric, Amount, Date) %>%
  filter(Metric %in% accounts_IS) %>%
  mutate(Metric = recode(Metric, 
  `Net Income (Loss) Available to Common Stockholders, Basic` 
  = "Net Income"))
IS_2016_2017_plot
```

Now, a nice plot.

```{r}
library(ggplot2)
g <- ggplot(IS_2016_2017_plot, 
            aes(Metric, Amount, fill = Amount)) +
  ggtitle("Facebook - Income statements in USD") +
  geom_bar(stat = "identity") + 
  scale_fill_gradient(low = "red", high = "blue")+
  facet_wrap(~ Date)
g
```

This is why we are interested firms to perform well in the economy. If firms are well managed, they are expected to take good strategic decisions to increase the value of the firm. In particular, the firm looks for funds from banks or owners, then invests in assets to produce or provide a service to the market. If things go well, sales increase and if revenues are greater than costs, the firm makes profits. When the firm increases their assets, they might also need more people to produce or manage those assets, then employment increases. More jobs mean more families with available income, and families will spend their income and stimulate the whole economy further. Even government may increase their tax revenues to (hopefully) spend them into high-quality public goods to increase society's welfare. Not only that, as a firm performs well, the whole value chain straightens as the firm requires inputs and services from other firms to operate. We are interested firms to perform well in the economy because that is good for the population in general and to other firms as well. Private investment stimulates the whole economy.

Strictly speaking, firms (regardless of their size) do not own what they have. Firm's assets were originally bought or financed using others' money (liabilities) and/or owners' money (equity). In simple terms, people are interested to finance the firm activities because the firm is supposed to know how to invest those funds in productive assets to make profits as in the case of Facebook. When the financial statements show that firms take bad decisions, their assets do not produce profits and people will not be very interested in financing a non-profitable firm. In good times, people who finance productive firms receive interests, returns, or dividends as a compensation for putting their money at risk.

Firms have incentives to be and keep being financially healthy in the future to attract financing more easily. Public firms (as those participating in the stock market) are subject to strict regulations and public scrutiny so everybody can constantly evaluate the firm performance. Regulations do not prevent all frauds or bankruptcies, but they are intended to minimize irregular and illegal practices. We were able to freely download financial statements because regulators ask public firms to do so given some deadlines and specific guidelines. Yearly and quarterly financial statements are analyzed in detail to evaluate firms as projects, and the main interest is the relationship between risk and return, so people can make better investment decisions. The decision of financing a firm operation is risky, but the reward is supposed to be attractive enough to compensate that risk. In finance we are interested to measure how risky are the investment opportunities in the form of firms, projects, and financial assets in general. When we do a financial analysis, we also compare the performance of one company not only with respect to previous years, but also with respect to other firms, and also with respect to their own industry or sector. This is not difficult as the regulators ask public firms to publish their financial statements in a similar manner, so comparisons among different firms are valid.

Finance is about people taking decisions under uncertainty with the help of data analysis. Think in two kind of people in the world: those who have money but no ideas (investors), and those who have ideas but no money (firms or entrepreneurs). We refer to ideas as business ideas, innovations, business projects that require funds to get started. Those with ideas and money are rare in this world, and those without ideas and without money are in trouble. When the financial system works well, productive projects will succeed at attracting funds, firms will invest in productive assets, private investment grows, and we should expect economic growth, and hopefully economic development improves. When the financial system fails, good projects might fail to get funding and probably bad projects get financing. This could be frustrating.

People with money and no ideas are looking for opportunities for their money to grow. They basically face two alternatives: to save their money (put it in the bank and receive a small but certain return), or invest their money in risky projects offered by firms or entrepreneurs. The final decision is taken based on a risk-return analysis, with the help of data analysis and financial models. Imagine the bank offers a secure 4%, and the firm offers a risky 3.5% return (assume it could be as low as −1% or as high as 4%). In that case, most of us might agree (if you are risk averse as me) that the funds are better in a bank. Now assume a different situation. The bank offers a secure 4%, and the firm offers a risky 6% return (it could be as low as 3.5% and as high as 8%). In that case, there might be some investors that after doing some analysis finally decides to put their money at risk. Things become interesting and complex because the same project can be attractive for some investors and bad for others as they might have different tools to analyze risky projects and they might also have different risk appetite levels.

People with ideas but no money are looking for funds to run their projects. If the project is impressively good, then the firm could ask for funds in exchange of a reduced return. The project is then not too risky and the return is also low. On the other hand, if the project looks OK but entails a high risk, the firm or entrepreneur should be willing to pay high returns to investors to compensate for the associated risk. This is why risk and return move together, the higher the risk the higher the return and vice-versa. If this rule fails, then we are in trouble.

Interestingly, an excess of private investment is not always good for the economy. Imagine the interest rate (return of savings, and the cost of asking for a loan) is low and the economic perspectives look good. In that case, we have good conditions for the private investment to grow as it is cheap to ask for financing. In the extreme, families now have more money and they start spending a lot. Aggregate demand for all goods and services increase and consequently the prices of all goods and services increase as well. The country is experiencing an increase in the inflation levels. Then, even though people have money in their pockets, they are not able to buy as prices increase faster than families' income. In order to reduce inflation levels, the monetary authority increases the interest rates, making savings more attractive and investment less attractive. If they succeed, people will spend less and save more. This decreases the aggregate demand for goods and services and prices could decrease again to desired levels. To add more drama, imagine that this increase in interest rates is so high that now economic growth is slowing down and the risk of falling into a recession increase. If that happens, then the monetary authority should decrease the interest rates again to incentive people to stop saving and start investing again. This is why you see that interest rates go up and down, although we have had low interest rates for a while.

Let's go back to finance and illustrate the process of a firm looking for funds, the difference between private and public firms, and see how a firm can grow or go bankrupt in a simple but illustrative example.

Firms can sell bonds or stocks as a way to get funds. Let's focus on the stocks. By selling stocks, the firm can increase their equity and consequently increase their assets. People who buy stocks expect the firm to make wise investment decisions to get a return in the future. Initially, in time zero $t=0$, assume the firm is private and has the following balance sheet:

```{r}
# The firm is private now. 
private_firm_bs <- c(assets = 100, liabilities = 40, equity = 60)
# Print results.
private_firm_bs
```

Private firms cannot sell stocks in the stock market, only public firms can. Selling stocks in the stock market is attractive for some firms because it is a way for them to grow. Stock markets allow more participant firms as long as they fulfill some strict requirements. Given the balance sheet above, the owner of this private firm invested 60 USD. When the firm goes 100% public, then the owner will sell the 60 USD investment to others.

In $t=1$, with the objective of getting fresh funds, the firm decides to go 100% public and sell 60 stocks at 1 USD each. Imagine that the firm succeeded at selling all 60 stocks at 1 USD. This is because investors who participate in the stock market consider that this is a fair value for the firm. These investors become new owners; this is why we say the firm is now public. So, by the end of $t=1$ we have:

```{r}
# The firm is public now. 
public_firm_bs <- c(assets = 100, liabilities = 40, 
                    equity_60x1 = 60)
# Print results.
public_firm_bs
```

In $t=2$ more investors realize that the firm is well managed, its market is growing and there is no close competition. Now, investors actually think that paying 1 USD per stock is actually a bargain, so they are willing to pay even more than 1 USD to participate in this project. So, the demand for those stocks increases, and as a result the price goes from 1 to 2 USD per share. By the end of $t=2$ we have:

```{r}
# Stock price increased from 1 to 2.
public_firm_bs <- c(assets = 100+60, liabilities = 40, 
                    equity_60x2 = 120)
# Print results.
public_firm_bs
```

Now the firm has grown by going public and by showing good perspectives.

However, in $t=3$ we sadly get to know that the firm's financial statements were overestimating profits, and a big fraud was detected today. Big scandal and all newspapers talk about it. Then, investors are now interested in selling all their stocks as quickly as they can. Investors are so desperate to sell their stocks that they drop the stock price to almost zero. By the end of $t=3$ we have:

```{r}
# Stock price decrease to 0.
public_firm_bs <- c(assets = 40, liabilities = 40, 
                    equity_60x0 = 0)
# Print results.
public_firm_bs
```

There are still 60 stocks in the market with a unit price of zero, so the market value of equity is null. This hypothetical firm is virtually bankrupt as they need to sell all their assets to pay the debt. In simple terms, a firm is bankrupt when the liabilities are greater than the assets. The firm will hardly get fresh funds as investors realize this was a bad project.

Many things can happen when a firm faces this difficult situation. Sometimes it is worthwhile to try to improve the firm's financial condition, hiring professional managers, and hope to recover the investor's confidence. Before bankruptcy, firms could also negotiate with their debt holders. If negotiation succeeds, the firm could get a longer debt maturity, a reduced debt to help the firm remain alive and hope to recover its operations, or a more innovative solution. There are also other firms that look for firms in trouble to buy them for a wide variety of reasons. In the US, you may hear about "Chapter 11", which is a form of bankruptcy that involves a reorganization of a debtor's business affairs, debts, and assets, and for that reason is known as "reorganization" bankruptcy. Corporations generally file Chapter 11 if they require time to restructure their debts. This version of bankruptcy gives the debtor a fresh start. However, the terms are subject to the debtor's fulfillment of its obligations under the plan of reorganization.

As we show before, the stock price evolution is generally a good indicator about how the investors interpret the future of the firm. When the stock price increases, then investors are demanding more stocks. A higher stock price is associated with a more valuable equity and this is good for the firm. When the stock price decreases, then there are more people trying to sell stocks compared to those trying to buy so the stock price falls. There might be some cases in which a drop in the stock price is associated with a firm good performance. Consider the stock price has increased in the last week, investors bought at 10 USD and today's price is 15 USD. Those investors might be interested to sell now to make a 5 USD profit. This increase in the supply of stocks could be significant enough to drop prices in the short run.

Remember this firm needed to go public before issuing and selling stocks in the stock market. Normally, these firms are big, although there are some stock markets for medium-sized firms. Small firms can hardly access these kinds of markets to get funding in a direct way. This might be problematic because small firms are the ones which require more alternatives to get financing so they can grow. The literature regarding small firms show that one of the most important problems they face is precisely the lack of financing or the limited alternatives to get funds. This remains an open challenge for the financial services and an opportunity for fintech companies. In the world there are significantly more small firms compared with big firms, so any improvement in the credit conditions of small firms could have a significant impact on the income of many people.

In the following section we move from a financial statement analysis to the analysis of stock prices. Note that both are linked by the equity and overall performance of public firms.

## Stock prices and other financial data.

Financial markets not only facilitate financial transactions between sellers and buyers, they also represent a rich source of financial data. Here, we show how to use R to download stock price data from financial markets by using a few examples.

R packages like *quantmod* and *tidyquant* make the process of downloading financial data to perform financial analysis very straightforward. The *quantmod* package for R is designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The *tidyquant* package is part of the so-called *tidyverse*, an opinionated collection of R packages introduced by Hadley Wickham and designed for data science. All *tidyverse* packages share an underlying design philosophy, grammar, and data structures.

In the past, when we were interested in performing a financial analysis in a rudimentary statistical software, we had to open the financial market site, download the data in Excel or text format, and then convert it to a compatible format given the statistical software requirements. In ancient times, people had to gather financial data from the newspaper. Fortunately for us, now we have R packages to make this process not only easy but also free, extremely efficient and immediate.

Let's download Apple stock prices in one step (we already loaded the *tidyquant* package as step zero).

```{r}
# Get stock prices for Apple stock from Yahoo! finance site.
aapl_stock_prices <- tq_get("AAPL")
```

We are done. The special variable \texttt{aapl\_stock\_prices} contains Apple stock prices and other relevant information. Many things happened here in a very few steps. First, we need to have Internet access, R and R Studio working on our computer. Then, we load the *tidyquant* package. This package presumably integrates the best resources for collecting and analyzing financial data in R. The \texttt{tq\_get} function belongs to the *tidyquant* package, and *tidyquant* belongs to the *tidyverse* packages. Specifically, \texttt{tq\_get} connects to a default financial site, which in this case is the finance.yahoo.com and looks for the "AAPL" symbol which corresponds to Apple Inc. (an American multinational technology company). Finally, we store the data into \texttt{aapl\_stock\_prices}.

To see what it is inside the \texttt{aapl\_stock\_prices} object we can do the following.

```{r}
# str function is a way to display the structure of an R object.
str(aapl_stock_prices)
```

According to the \texttt{str} output, \texttt{aapl\_stock\_prices}, we have `r nrow(aapl_stock_prices)` daily observations for 7 variables. As you can see, we have not only prices but also volume. Let's consider a different way to see and have a grasp of what it is inside \texttt{aapl\_stock\_prices}. In particular, we can see the first and the last part of this lengthy daily database.

The first observations.

```{r}
# See the first observations of aapl_stock_prices.
head(aapl_stock_prices)
```

The last observations.

```{r}
# See the last observations of aapl_stock_prices.
tail(aapl_stock_prices)
```

By default, the \texttt{tq\_get} function downloads the latest set of data available. You can verify that the last date of \texttt{aapl\_stock\_prices} above approximately corresponds to the production date of this tutorial. If there is a difference it is simply because the stock market is still close or we are running the code in a weekend. In this case:

-   Last \texttt{aapl\_stock\_prices} observation: `r aapl_stock_prices$date[length(aapl_stock_prices$date)]`.
-   Today is: `r Sys.Date()`.

We also like to show the data in a plot.

```{r}
# Date is in the x axis, price in the y axis. The type l stands for line.
plot(aapl_stock_prices$date, aapl_stock_prices$adjusted, 
     type = "l")
```

We can add a few instructions to improve the format of our plot.

```{r}
# x and y labels, the title, and the color of the line.
plot(aapl_stock_prices$date, aapl_stock_prices$adjusted, 
     type = "l",
     xlab = "Date", ylab = "Adjusted price", 
     main = "Apple stock price", col = "blue")
```

We can compare Apple versus a stock index like S&P500.

```{r}
# Download the S&P500 index.
SP <- tq_get("^GSPC")
```

Plot both assets together.

```{r}
par(mar = c(5, 5, 2, 5))
plot(SP$date, SP$adjusted, type = "l", col = "red",
     ylab = "Standard and Poors 500 index",
     xlab = "Date", 
     main = "Index and stock 2011-01-03 to 2021-03-05")
par(new = T)
plot(aapl_stock_prices$date, aapl_stock_prices$adjusted, 
     type = "l", axes = F, xlab = NA, ylab = NA, cex = 1.2)
axis(side = 4)
mtext(side = 4, line = 3, "Apple stock price")
legend("topleft",
       legend=c("SP500", "Apple"),
       lty = 1, col = c("red", "black"))
```

Let's continue with the Apple stock analysis. Stock prices change throughout the trading day. This is, the open price changes every minute (sometimes in milliseconds), so the open price is normally different from the closing price. We can calculate the difference between the high and low price of each day and then show the result in a plot.

```{r}
# Create a new variable.
aapl_diff = aapl_stock_prices$high - aapl_stock_prices$low
# Plot the new variable.
plot(aapl_stock_prices$date, aapl_diff, type = "h",
     xlab = "Date", 
     ylab = "Difference between high and low prices", 
     main = "Apple's stock price can change about $25 in one day",
     col = "blue")
```

The difference between high and low prices show that the stock price changes during a trading day. These changes can be considerably high. Moreover, this volatility has increased in recent times. Is it possible to know the exact date in which this difference is the highest? One alternative is to sort all observations, but there is a simpler approach.

```{r}
# Which observation has the maximum value of aapl_diff?
highest_change = which.max(aapl_diff)
# Show results.
highest_change
```

Consider the value of \texttt{highest\_change} as an index. This is, the number of the row that contains the highest \texttt{aapl\_diff} value. We can use this index to extract the date and the actual \texttt{aapl\_diff} value.

```{r}
# Now, extract the date and the actual value of aapl_diff.
# See how brackets [] are used to extract a single row value.
when = aapl_stock_prices$date[highest_change]
top = aapl_diff[highest_change]
```

The output above shows when (in the variable \texttt{when}) Apple stock had its highest price change (\texttt{top}) in one day.

```{r}
# We can use when and top variables to improve our plot.
plot(aapl_stock_prices$date, aapl_diff, type = "h",
     xlab = "Date", 
     ylab = "Difference between high and low prices", 
     main = "Apple's stock price can change about $12 in one day",
     col = "blue")
# Here, we add the red point.
points(when, top, pch = 19, col = "red", cex = 1.5)
```

Up to know, we have been using default options as we simply ask for stock prices of Apple without any other kind of instruction. As a result, we get data starting from 2010: \texttt{tq\_get("AAPL")}. However, we can change default options if needed to get data back to 1990. I will take some online available examples developed by the author of *tidyquant* Matt Dancho.

```{r}
# Download Apple stock prices.
aapl_prices  <- tq_get("AAPL", get = "stock.prices", 
                       from = " 1990-01-01")
# Show results.
aapl_prices
```

Sometimes we are interested in periodicity aggregation from daily to monthly. We cannot transform from monthly to daily, but we can always transform from daily to monthly, monthly to yearly and so on. FANG is a dataset containing the daily historical stock prices for the "FANG" tech stocks, "FB", "AMZN", "NFLX", and "GOOG", spanning from the beginning of 2013 through the end of 2016.

Let's aggregate data from daily to monthly frequency.

```{r}
# Here, we are using a pre-loaded dataset.
data("FANG")
# Transform from daily to monthly stock prices.
FANG %>%
    group_by(symbol) %>%
    tq_transmute(select = adjusted, mutate_fun = to.monthly, 
                 indexAt = "lastof")
```

The *tidyquant* package can also access other kinds of data from diverse sources like the Federal Reserve Economic Data (FRED). Federal Reserve Economic Data is a database maintained by the Research division of the Federal Reserve Bank of St. Louis that has more than 765,000 economic time series from 96 sources. Consider the WTI Crude Oil Prices.

```{r}
# See https://fred.stlouisfed.org/series/DCOILWTICO
# Download oil prices from FRED.
wti_price_usd <- tq_get("DCOILWTICO", get = "economic.data")
# Show results.
wti_price_usd
```

Now that we have oil prices in basically one single step, we can plot them. It is interesting because recently the oil prices have turned negative in one day. Negative prices are rare but not impossible, especially in commodities.

```{r}
# ggplot2 is a system for creating graphics see https://amzn.to/2ef1eWp.
library(ggplot2)
# Create a simple line plot to show the oil price evolution.
ggplot(wti_price_usd, aes(date, price)) + 
  geom_line() + theme_bw() +
geom_hline(yintercept = 0, linetype = "dashed", color = "red")
```

It is easy to find out the exact day when the oil price was negative.

```{r}
# Extract one row given a condition.
subset(wti_price_usd, price < 0)
```

Without going into technical arguments, we can interpret negative prices as the following. Imagine (as it actually happened) that storing oil is expensive, not only that, imagine oil producers have no further physical space to store oil, so they are desperate to get rid of the oil production. On the other hand, oil buyers realize that the economic perspectives in the world look bad. If the economic activity suddenly stops, then we expect a lower demand for fuel including oil as firms are producing less. Then, producers want to sell and buyers are not interested to buy as they do not need as countries have more than enough inventories. This could go to the extreme in which producers are (sadly) willing to pay people in exchange of taking the oil out of their hands. This is why commodity prices can be negative.

There are other explanations, for example one related to the maturity of oil futures contracts. The price that went negative on Monday 2020-04-20 was for futures contracts to be delivered in May. Those contracts expired on Tuesday 2020-04-21. Upon expiration of the futures contract, the clearinghouse matches the holder of a long contract against the holder of a short position. The short position delivers the underlying asset to the long position. So, on Monday, traders --- who were not equipped to take physical deliveries --- were rushing to sell them to buyers who have booked storage.

The *quantmod* package can retrieve exchange rates easily.

```{r}
# Load the package.
library(quantmod) 
# Download USD/MXN exchange rate from Oanda site.
exchange_rate <- getSymbols("USD/MXN", src = "oanda", 
                            auto.assign = FALSE)
# Plot the results.
plot(exchange_rate)
```

Here, 1 USD equals `r last(exchange_rate)` MXN as `r Sys.Date()`. We can show the time series information into a density plot.

```{r}
# Density plots.
ggplot(exchange_rate, aes(x = USD.MXN, fill = "")) +
  geom_density(alpha = 0.8) +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  labs(title = "USD.MXN", x = "USD.MXN", y = "Density") +
  theme(legend.position = "none", legend.title = element_blank()) +
  scale_fill_tq()
```

We can conveniently download the data directly from the FRED API. Let's see data of "Beer, Wine, and Distilled Alcoholic Beverages Sales". For the full database details see:

<https://fred.stlouisfed.org/series/S4248SM144NCEN>

```{r}
# Beer, Wine, Distilled Alcoholic Beverages, in Millions USD
beer_sales_tbl <- tq_get("S4248SM144NCEN", get = "economic.data", 
                         from = "2010-01-01", to = "2021-01-01")
```

Let's have a look at the data set. By default, it says *price*, but these are basically sales figures. According to the main FRED reference, these are in millions of dollars, not seasonally adjusted.

```{r}
# See part of the data.
glimpse(beer_sales_tbl)
```

Visualization is particularly important for time series analysis and forecasting. It is also important to see the time series because normally the models will perform better if we can identify time series basic characteristics such as trend and seasonality. This data set clearly has a trend and a seasonality.

```{r}
# Plot the data.
beer_sales_tbl %>%
  ggplot(aes(date, price)) + 
  geom_line(col = palette_light()[1]) +
  geom_point(col = palette_light()[1]) + theme_tq() +
  scale_x_date(date_breaks = "1 year", date_labels = "%Y") +
  labs(title = "Beer Sales: 2010 through 2020.") 
```

A time series can be decomposed as shown below.

```{r}
library(fpp3)

stl_dcmp <- beer_sales_tbl %>% 
  mutate(date = yearmonth(date)) %>%
  as_tsibble(index = date)

stl_dcmp %>%
  model(STL(price ~ trend(window = 7) + 
              season(window = "periodic"), robust = TRUE)) %>%
  components() %>%
  autoplot()
```

We can also create some interesting visualizations about US employment and the US recessions (in the shaded area) over time. The code below is not so compact but it works.

```{r}
# NBER Recession indicator and US nonfarm payroll employment
tickers<- c("USREC", "PAYEMS")
df <- tq_get(tickers, get = "economic.data", from = "1948-01-01")
# recession df (for plotting)
recessions.df = read.table(textConnection(
  "Peak, Trough
  1945-02-01, 1945-10-01
  1948-11-01, 1949-10-01
  1953-07-01, 1954-05-01
  1957-08-01, 1958-04-01
  1960-04-01, 1961-02-01
  1969-12-01, 1970-11-01
  1973-11-01, 1975-03-01
  1980-01-01, 1980-07-01
  1981-07-01, 1982-11-01
  1990-07-01, 1991-03-01
  2001-03-01, 2001-11-01
  2007-12-01, 2009-06-01
  2020-02-01, 2021-01-30"), sep = ',', 
  colClasses = c('Date', 'Date'), header = TRUE)
rec3 <- filter(df, df$symbol == "PAYEMS")
my_trans <- function(in.data,transform = "pctdiff3") {
  switch(transform, logdiff  = c(NA, diff(log(in.data))),       
         pctdiff3 = 100 * Delt(in.data, k = 3),
        logdiff3 = c(rep(NA, 3), diff(log(in.data), 3)))
}
vlist <- c("PAYEMS")
df2 <- df %>% group_by(symbol) %>%
  mutate(x = ifelse(symbol %in% vlist, my_trans(price), price))
df2 %>% select(-price)  %>% spread(symbol, x) %>%
  mutate(REC12 = lead(USREC, 12)) -> df3              
df41 <- filter(df3, year(date) > 1945)
```

We have the data, now we can plot.

```{r}
ggplot(data = df41, aes(x = date, y = PAYEMS)) + 
  geom_rect(data = recessions.df, inherit.aes = FALSE,
            aes(xmin = Peak, xmax = Trough, 
                ymin = -Inf, ymax = +Inf), 
            fill = 'black', alpha = 0.5) + theme_minimal() + 
  geom_line(color = "red", size = 1.5) + 
  labs(x = "", y = "", 
       title = "US employment growth and recessions.",
       subtitle = "Last value: December 2020. Shaded area are NBER recessions.", 
       caption = "Source: US Bureau of Labor Statistics.
       \nretrieved from FRED, Federal Reserve Bank of St. Louis.") + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = as.numeric(as.Date("2020-12-01")), 
             type = 3) +
  theme(plot.caption = element_text(hjust = 0),
        plot.subtitle = element_text(face = "italic", size = 9),
        plot.title = element_text(face = "bold", size = 14))
```

Here we can see the magnitude of the current and upcoming financial and economic crisis. The fall in the US employment is impressive as you can see. Note how recessions (gray bars) are very closely related with falls in the employment over time.

Given the current (2020) health crisis many sectors and firms in the US economy stopped or reduced operations as a way to reduce the spread of the virus. As firms produce less (or nothing), they sell less (or nothing). Firms face two kinds of costs, variable and fixed. Variable costs depend on production, so if production decreases the variable costs decreases as well. The problem here is fixed costs because they never disappear, they are fixed and they have to be paid no matter what. Some firms might have the possibility to pay fixed costs with reduced or null revenues for a while (days, weeks, probably a little bit more), but definitely not for long. I believe it is clear what happen next. Some firms did not make it, they could not survive and they simply went bankrupt and definitely closed operations. These firms had employees and now they are unemployed. Even those firms who are still operating, in some cases they had to reduce the payroll, and some employees are now unemployed. Less aggregate production and more families without an income reduce the aggregate supply and demand. This is why this current-next economic crisis has no precedent.

Having easy access to financial and economic data is important to facilitate the data analysis. However, it is even more important to be able to manipulate the financial and economic data correctly to communicate facts and discover new insights to make better decisions.

## Technical analysis.

Technical analysis is the forecasting of future financial price movements based on an examination of past price movements and volume of trading. Technical analysis is applicable to stocks, indexes, commodities, futures or any tradable instrument where the price is influenced by the forces of supply and demand. This analysis is limited because it fails to incorporate other forces and factors that can influence the price of the security. Although this analysis is limited, it is still quite popular, and given that we now know how to get financial prices, it seems convenient to illustrate the basics of technical analysis.

Let's use FANG data again.

```{r}
# Get AAPL and AMZN stock prices.
AAPL <- tq_get("AAPL", get = "stock.prices", from = "2015-09-01", 
               to = "2016-12-31")
```

Let's begin with one plot.

```{r}
# Plot the data.
AAPL %>%
    ggplot(aes(x = date, y = close)) + geom_line() +
    labs(title = "Apple Line Chart", 
         y = "Closing Price", x = "") +
    theme_tq()
```

The plot above is about closing prices. However, we also have open and close prices per day. We could take advantage of this by plotting vertical lines per day. The length of the daily vertical lines represents the difference between the open and close prices. To make it more informative, blue lines are those cases when the close price is greater than the open price, and red lines are those cases when the close price is lower than the open price.

```{r}
# Plot the data.
AAPL %>%
    ggplot(aes(x = date, y = close)) +
    geom_candlestick(aes(open = open, high = high, low = low, 
                         close = close)) +
    labs(title = "AAPL Candlestick Chart", y = "Closing Price", 
         x = "") + theme_tq()
```

There are some blank spaces. This is simply because the close price of yesterday is not always exactly the same as the open price of today. Local stock markets close on weekends, and there are also holidays.

These kinds of charts are difficult to read when we have many observations. Let's do a zoom:

```{r}
# A candlestick plot.
AAPL %>%
    ggplot(aes(x = date, y = close)) +
    geom_candlestick(aes(open = open, high = high, low = low, 
                         close = close)) +
  coord_x_date(xlim = c("2016-12-15", "2016-12-31"), 
               ylim = c(28.75, 29.5)) +
    labs(title = "AAPL Candlestick Chart (zoom)", 
         y = "Closing Price", x = "") + theme_tq()
```

A formal interpretation is difficult given the limitations of this analysis. However, the main idea is the following. Consider the stock is increasing quickly, then it is believed that before a fall in the price, the stock will increase at a lower speed. Big blue bars are eventually followed by smaller blue bars before exhibiting a price fall (a red bar).[^4] The same logic can be applied the other way around. A dramatic fall in the price eventually reach a floor. This is, big red bars are eventually followed by smaller red bars before experiencing a price increase.

[^4]: The logic behind this is not very different from what we have heard about "flatten the curve" in the context of the 2020 pandemic. As the curve increases, the high speed of increase is captured by big vertical blue bars. It is expected that the speed of increase slows down before the curve is flat, and this slowing down is captured by smaller vertical blue lines. So, before the curve is flat and starts to decrease, the blue vertical lines become smaller and smaller.

Could you guess what will happen with the price given the chart above? Seems difficult as the last four observations are consecutive blue-red-blue-red. Then, this indicator alone, without any complementary analysis, does not seem to deliver a clear signal.

Moving averages are supposed to anticipate price movements. The idea is the following. We have two line plots, one is the original evolution of the price chart in black and the second is the moving average in blue. In this case we have a 30-day moving average. The blue line is simply the average of the last 30-day stock prices. In the code below, this value is easily modified by changing the value of $n$, in this case $n=30$. The blue line (moving average) seems to anticipate what happens to the black line. The way we interpret the moving average is simple. If the black line crosses the blue from top to down in $t$, then we are expected to anticipate a drop in the stock price in $t+1$. In case the black line remains below the blue after $t+1$, then we expect the stock price to keep falling in the following periods.

If the black line crosses the blue from down to top, then we are supposed to anticipate an increase in the stock price. The sign is stronger when the black line crosses the blue line showing a steeper slope. What happens in practice is that the value of $n$ should be calibrated until we fit the moving average with the behaviour of the price stock. This indicator alone is far from being perfect as there might be a lot of contrary signals which might make it difficult to follow the interpretation as we explained above.

```{r}
# The simple moving average (SMA) plot. 
AAPL %>%
    ggplot(aes(x = date, y = close)) +
    geom_line(aes(open = open, high = high, low = low, 
                         close = close))  +
    geom_ma(ma_fun = SMA, n = 30, linetype = 1, size = 1.25) +
    labs(title = "Apple Moving average chart",
         subtitle = "30-Day SMA",
         y = "Closing Price", x = "") + theme_tq()
```

There is a price fall a few weeks before January 2016 and a few weeks after January 2016. This price fall was correctly anticipated by the moving average. Then the stock price increased a bit more than 110 USD, this was also correctly anticipated by the moving average. The period around July 2016 is less clear. By the end of the time-series, the black line is above the blue, so trying to follow our previous explanation this means that we should not expect a significant drop in the stock price.

```{r}
AAPL_recent <- tq_get("AAPL", get = "stock.prices", 
                      from = "2016-12-30", to = "2017-01-12")
AAPL_recent
```

The Bollinger Bands are envelopes plotted at a standard deviation level above and below a simple moving average of the price. Because the distance of the bands is based on standard deviation, they are supposed to adjust to volatility swings in the underlying price. This indicator can help us to understand the size of the expected change in the future. If the Bollinger Bands become wider then we should expect drastic changes in the stock price.

```{r}
# Bollinger bands and simple moving average.
AAPL %>%
    ggplot(aes(x = date, y = close, open = open,
               high = high, low = low, close = close)) +
    geom_line() +
    geom_bbands(ma_fun = SMA, sd = 2, n = 30,
                linetype = 2, size = 1, alpha = 0.2,
                fill        = palette_light()[[1]],
                color_bands = palette_light()[[1]],
                color_ma    = palette_light()[[2]]) +
    labs(title = "BBands with SMA", y = "Closing Price", x = "") +
    theme_tq()
```

Let's look at the very last observation. The price line is above the moving average (now in red, dotted line). This means that we should not expect a drop in the stock price in the following periods. However, the Bollinger Bands suggest that the price is currently very close to the upper bound, plus the upper bounds is also close to the historical maximum price at least in this plot. In sum, the last price is 115.82 USD and we do not expect a fall in the stock price according to the moving average and the expected price change is in the range of 107 to 119 USD approximately.

In case you may have a different view or concerns about the previous interpretation then you now realize the pitfalls and the disadvantages of this kind of analysis when it is conducted as a simple chart read. Although, the technical analysis is popular, it could lead to different interpretations easily when we interpret one or two indicators by eye. This does not mean we have to ignore this kind of analysis because we can always incorporate formal techniques to try to validate buy and sell signals to help traders and conduct short-run investment strategies. In fact, there are very serious developments like the *quantstrat* package, which provides a generic infrastructure to model and backtest signal-based quantitative strategies.

We can show the simple moving average for Facebook.

```{r}
# The simple moving average (SMA) plot. 
FANG %>%
  filter(symbol == "FB") %>%
  ggplot(aes(x = date, y = close)) +
  geom_line(aes(open = open, high = high, low = low, 
                close = close))  +
  geom_ma(ma_fun = SMA, n = 60, linetype = 1, size = 1.25) +
  labs(title = "Facebook Moving average chart",
       subtitle = "30-Day SMA",
       y = "Closing Price", x = "") + theme_tq()
```

This suggests that the stock price will decrease in the near future. Let's see if the moving average signal is correct.

```{r}
FANG %>%
  filter(symbol == "FB") %>%
  filter(date >= "2014-01-01" & date <= "2014-02-01")
```

The moving average sent the right stock price movement signal.

# Asset returns.

An asset is any resource owned by a business or an economic entity. It is anything that can produce value (or returns) in the future. Here, we frequently use firm stocks as financial assets, but we also show other examples of assets such as commodities (oil), currencies, bonds, derivatives, etc.

A return is the percentage change of an asset price. In finance, we are very interested in asset returns because it allows us to measure asset (like stocks) performance in relative terms. Consider the following example. In (1) I buy at 100 USD and sell at 110 USD; and in (2) I buy at 10 USD and sell at 12 USD. I make money in both situations, that is for sure because $110 > 100$ and $12 > 10$. But which situation was the best deal? In absolute terms, I made 10 USD in (1) and only 2 USD in (2). However, in relative terms I made 10% in (1) and a great 20% in (2).

The previous returns were calculated as a simple percentage change: $(110-100)/100$ and $(12-10)/10$ respectively. We can calculate log-returns instead and these are equivalent, and sometimes we prefer to calculate log-returns especially when we deal with short periods of time. In this case the log-returns are $log(110/100)$ and $log(12/10)$ which are 9.53% and 18.23% respectively.

Both absolute and relative valuations (10 USD versus 2 USD, and 10% versus 20%) are important and valid depending on what we want to report. However, the good thing about the relative approach (percentages) is that we can compare among different alternatives more easily. Returns also have some relevant statistical properties (like stationarity) that are needed to implement some quantitative financial and econometrics models.

Even returns can be expressed in relative terms with respect to the associated asset risk. Consider another example. In (1) I have an asset with an expected return of 5% with a risk of 5%; and in (2) a different asset with an expected return of 5% with a risk of 10%. So, in relative terms (1) is an asset with 1 unit of return per unit of risk and (2) is an asset with 0.5 unit of return per unit of risk. Now, it is clear we should prefer alternative (1).

There are more technical reasons to work with returns, but we can proceed with our analysis. Also, there are other ways to measure and report asset returns including different frequencies, cumulative returns, compounded returns, etc., but that is something we can discuss later.

## From prices to returns.

We need time series of asset prices to calculate time series of asset returns. Let's begin with a visual representation of the FANG database. Here, we have daily stock prices per stock.

```{r}
# Plot the data.
FANG_daily_all <- FANG %>%
    group_by(symbol) %>%
    ggplot(aes(x = date, y = adjusted, color = symbol)) +
    geom_line(size = 1) +
    labs(title = "Daily stock prices - not so easy to read.",
         x = "", y = "Adjusted prices", color = "") +
    scale_y_continuous(labels = scales::dollar) +
    theme_tq() + scale_color_tq()
FANG_daily_all
```

See how stock prices fluctuate in different ranges. It would be a mistake to prefer Amazon or Google simply because they experience larger price changes compared with Facebook or Netflix as we discuss in the previous section. Taking investment decisions based on prices could be misleading because investors are most of the time looking for returns, and in particular for an attractive risk-return combination. The vertical axis is also problematic in the plot as it is difficult to see the price evolution clearly for each stock. In sum, we do not take investment decisions based on this kind of visual representation. If we are interested to evaluate the stock performance, we need a risk-return analysis, not a price chart.

Let's add one line of code to see the price time-series more clearly.

```{r}
# Facet makes it easier to read.
FANG_daily <- FANG %>%
    group_by(symbol) %>%
    ggplot(aes(x = date, y = adjusted, color = symbol)) +
    geom_line(size = 1) +
    labs(title = "Daily Stock Prices - easier to read",
         x = "", y = "Adjusted Prices", color = "") +
    facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
    scale_y_continuous(labels = scales::dollar) +
    theme_tq() + scale_color_tq()
FANG_daily
```

Now it is clear that all stock prices have a positive trend over time. It is also clear that 2014 was a bad year for Amazon, Google and even Netflix. This is because the stock price decreased in the period of 2014-2015. The year 2015 was good for Amazon and Google. The main point is that this new visualization of the data can help us to compare the evolution of price per stock. Remember we still have prices and not returns. Stock returns are easily calculated in R.

Here, we first select adjusted prices, then transform into yearly returns.

```{r}
# Create yearly returns.
FANG_annual_returns <- FANG %>%
    group_by(symbol) %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "yearly",
                 type       = "arithmetic")
FANG_annual_returns
```

This is what we call tidy data because (1) each variable forms a column; (2) each observation forms a row; and (3) each type of observational unit forms a table. These yearly returns are expressed in decimal notation. This means that 0.95 is in reality 95%.

The next chunk of code shows how to visualize the annual returns in a nice plot.

```{r}
# Plot the annual returns.
FANG_annual_returns %>%
    ggplot(aes(x = date-365, y = yearly.returns, fill = symbol)) +
    geom_col() +
    geom_hline(yintercept = 0, color = palette_light()[[1]]) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = "FANG: Annual Returns",
         y = "Annual Returns", x = "") +
    facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
    theme_tq() + scale_fill_tq()
```

Risky stocks like these are risky because the returns change drastically from time to time. Note that here, it is easy to see that 2014 was indeed a bad year for Amazon, Google and Netflix as we anticipate when looking at the price chart. These three assets confirm what we anticipate before because we see negative returns in 2014. The year 2015 was good for Amazon and Google as we said before.

The question that arises now is: What would be the result of investing money in each stock during these years (starting the first day of 2013 and finishing the last day of 2016)? Yearly returns show what happens from one year to another, but we cannot tell which stock would represent the best investment strategy in these years looking at this past information. Cumulative returns can be useful to answer this question.

## Cumulative returns.

Let's start with a brief note about working with quantities affected by percentage changes. My salary is 100 USD and I manage to get a 5% increase. Then my new salary is $100 \times (1+0.05) = 105$. If I do $100 \times 0.05 = 5$ I only get the net increase, but what I care about is my new salary. Now I negotiate to change from full-time to part-time and agree to a salary decrease of 20%. Then, my current salary is $100 \times (1+0.05) \times (1-0.2) = 84$.

Imagine we invest 100 USD in Amazon for this period. Let's calculate the value of our investment considering that we receive the four yearly returns listed and shown above. Remember the Amazon's yearly returns from 2013 to 2016 are: 54.98%, −22.18%, 117.78%, 10.95% in this specific order.

```{r}
# Investment in USD.
investment = 100
# Amazon's cumulative return in percentage.
AMZN_cum_percentage = (1+0.54984265)*(1-0.22177086)*
  (1+1.17783149)*(1+0.10945565) - 1 
# Amazon's cumulative return in USD.
AMZN_cum_usd = investment * (1 + AMZN_cum_percentage)
# Show results.
AMZN_cum_percentage
AMZN_cum_usd
```

The dollar value of 100 USD invested in Amazon at the end of the 4 years is 291.4267 USD. The cumulative return is 191.4267%. For the sake of clarity, we can check these values are correct by applying this return over the 100 USD: $100 \times (1+1.914267) = 291.4267$.

The 191.4267% cumulative return is usually not reported as it is here because it accumulates 4 years of returns. We normally express mean returns, in this case mean yearly return. The arithmetic mean of the returns, calculated by taking the sum of the returns and dividing by 4 is:

```{r}
# Arithmetic mean return.
AMZN_arith_mean_retun = (0.54984265 - 0.22177086 + 
                           1.17783149 + 0.10945565) / 4
AMZN_arith_mean_retun
```

Let's reveal a problem with returns calculated as an arithmetic mean that it is sometimes disregarded. In our example, the arithmetic mean of returns is 40.38397% per year. Note that this could be misleading if we calculate the growth of a 100 USD investment for four years:

```{r}
# The long way.
investment * (1 + AMZN_arith_mean_retun) * 
  (1 + AMZN_arith_mean_retun) * 
  (1 + AMZN_arith_mean_retun) * (1 + AMZN_arith_mean_retun)
# The short way.
investment * (1 + AMZN_arith_mean_retun) ^ 4
```

See something strange? We know that the dollar value of 100 USD invested in Amazon at the end of the 4 years is 291.4267 USD. However, taking this mean arithmetic return we have 388.3919 USD. See how problematic this could be. I can say my 100 USD investment in Amazon during the past 4 years was great because I get a mean return of 40.38397% per year. People might think that my money grew from 100 USD to 388.3919 USD by the end of the fourth year, whereas in reality my money grew from 100 USD to 291.4267 USD.

The way we reconcile this difference is by calculating a geometric mean return rather than an arithmetic mean return. A geometric mean is the $n$th root of the product of $n$ numbers. In particular:

```{r}
# Geometric mean return.
AMZN_geom_mean_retun = ((1+0.54984265)*(1-0.22177086)*
  (1+1.17783149)*(1+0.10945565)) ^ (1/4) - 1
AMZN_geom_mean_retun
```

I can say my 100 USD investment in Amazon during the past 4 years was great because I get a mean (geometric) return of 30.65689% per year. People might think that my money grew from 100 USD to 291.4267 USD by the end of the fourth year, and this is correct. Let's verify the procedure.

```{r}
# The long way.
investment * (1 + AMZN_geom_mean_retun) * 
  (1 + AMZN_geom_mean_retun) * 
  (1 + AMZN_geom_mean_retun) * (1 + AMZN_geom_mean_retun)
# The short way.
investment * (1 + AMZN_geom_mean_retun) ^ 4
```

This phenomenon is an example of a result that is well known in mathematics. The geometric mean of a set of numbers is always less than the arithmetic mean (30.6 \< 40.3). Although our arithmetic mean return of 40.38397% can lead to an overestimation of the investment dollar returns, we usually report mean returns. This is a common issue when reporting mutual funds returns as it is tempting to report arithmetic mean returns because they are higher. This is why in some jurisdictions, regulations require fund managers to report geometric mean returns instead (30.65689% in this case).

For now, we will work with arithmetic returns. Let's see How 100 USD investment growth each year per stock. Note that the plotted values are in USD.

```{r}
# Yearly returns.
FANG_annual_returns <- FANG %>%
    group_by(symbol) %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "yearly",
                 type       = "arithmetic")
# Cumulative returns.
FANG_annual_cum_returns <- FANG_annual_returns %>%
  mutate(cr = 100*cumprod(1 + yearly.returns)) %>%
# Plot the results.
  ggplot(aes(x = date-365, y = cr, fill = symbol)) + geom_col() + 
  labs(title = "Yearly cumulative USD returns.",
  subtitle = "100 USD investment growth from 2013-01-01 to 2016-12-31.",
  x = "", y = "USD value at the end of the year", color = "") +
  scale_y_continuous(labels = scales::dollar) +
  facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
  theme_tq() + scale_color_tq()
FANG_annual_cum_returns
```

Please verify that the dollar value of 100 USD invested in Amazon at the end of the 4 years is 291.4267 USD, as we discussed before.

We can also modify the code above to show monthly returns (in percentage) instead of yearly.

```{r}
# Monthly returns.
FANG_monthly_returns <- FANG %>%
    group_by(symbol) %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "monthly",
                 type       = "arithmetic")
# Plot the results.
ggplot(FANG_monthly_returns, aes(x = date-12, 
       y = monthly.returns, fill = symbol)) +
  geom_col() +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "FANG: Monthly Returns", y = "Monthly Returns", 
       x = "") +
  facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
  theme_tq() + scale_fill_tq()
```

By increasing the frequency from yearly to monthly we now have 12 observations per month instead of 1. Note that this new plot reveals how volatile these stock returns are. There is a negative return just the month after GOOG reached a positive 20% return. It looks hard to anticipate what will happen the next month since there is evidence that these returns change from positive to negative quite frequently.

Let's see cumulative monthly returns per stock to extend our 100 USD investment example.

```{r}
# Calculate monthly cumulative returns.
FANG_monthly_cum_returns <- FANG_monthly_returns %>%
  mutate(cr = 100 * cumprod(1 + monthly.returns))
# Plot results.
ggplot(FANG_monthly_cum_returns, aes(x = date-12, y = cr, 
                                     color = symbol)) +
  geom_line(size = 1) +
  labs(title = "Monthly cumulative USD returns.",
subtitle = "100 USD investment growth from 2013-01-01 to 2016-12-31.",
         x = "", y = "USD value at the end of the year", 
color = "") +
  scale_y_continuous(labels = scales::dollar) +
  theme_tq() + scale_color_tq()
```

Alternatively, we can split the plot in four panels. This facilitates the reading of the plot.

```{r}
# Plot results.
ggplot(FANG_monthly_cum_returns, aes(x = date-12, y = cr, 
                                     color = symbol)) +
  geom_line(size = 1) +
  labs(title = "Monthly cumulative USD returns.",
subtitle = "100 USD investment growth from 2013-01-01 to 2016-12-31.",
         x = "", y = "USD value at the end of the year", 
color = "") +
  scale_y_continuous(labels = scales::dollar) +
  facet_wrap(~ symbol, ncol = 2, scales = "free_y") +
  theme_tq() + scale_color_tq()
```

Again, please verify that the dollar value of 100 USD invested in Amazon at the end of the 4 years is 291.4267 USD, as we discussed before. Here, we can see the USD evolution of my investment every month.

Remember this plot?

```{r}
# FANG stock prices.
FANG_daily_all
```

Netflix stock price remains down all the time. However, 100 USD invested in Netflix during these 4 year would lead to the highest value of almost 1000 USD. This is why we should conduct a return analysis to make investment decisions.

Plotting prices and returns over time is revealing because we can see the evolution of the stock performance over time. It is useful because we could analyze what happens on specific dates to better understand the behaviour of stocks. Remember 2014 was a bad year for Amazon and Google, it would be interesting to investigate what went wrong then so we can better understand the determinants of the firm's value.

## Distribution of returns.

Although time series plots represent a useful tool for financial analysis, it is not the only way we can show stock returns. Density plots forget about time and show the distribution of values. The height (vertical axis) represents how frequent a given return (horizontal axis) is. This approach is useful when we are interested to know the most likely return that the stock can have, how unlikely is to expect determined return values, how risky a specific stock return is.

Let's visualize a density plot for the FENG data.

```{r}
# Density plots.
ggplot(FANG_monthly_returns, aes(x = monthly.returns, 
                                 fill = symbol)) +
  geom_density(alpha = 0.5) +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  labs(title = "FANG: Charting the Monthly Returns",
       x = "Monthly Returns", y = "Density") + xlim(-0.3, 0.9) +
    theme_tq() + scale_fill_tq()
```

Note that monthly returns around zero are frequent, whereas monthly returns above 50% are very infrequent. Apparently, this 50% value is red, so this should be the case of Facebook. It is also noteworthy to mention that Netflix has the higher monthly return dispersion of all. If this is not clear enough, we can add one line to the code to make it clearer.

```{r}
# Density plots.
ggplot(FANG_monthly_returns, aes(x = monthly.returns, 
                                 fill = symbol)) +
  geom_density(alpha = 0.5) +
  geom_hline(yintercept = 0, color = palette_light()[[1]]) +
  labs(title = "FANG: Charting the Monthly Returns",
       x = "Monthly Returns", y = "Density") + xlim(-0.3, 0.9) +
  theme_tq() + scale_fill_tq() + facet_wrap(~ symbol, ncol = 2)
```

Now, we have one density plot for each stock. Let's analyze each plot.

-   AMZN. Note the highest value is zero, this suggests that the most frequent return value is around zero. The majority of observations are within −25% and 25%, and there are only a very few returns higher than 25%.
-   GOOG. As in the case of AMZN, the most frequent return value is around zero. Contrary to AMZN, there are no monthly returns higher than 25%. GOOG shows a less dispersed distribution so it is less risky than AMZN.
-   FB. The highest value is slightly above zero, this means that the average return should be higher compared with AMZN and GOOG. However, FB exhibits a greater dispersion of returns as there are a few around 50%.
-   NFLX. This is the most riskiest asset compared with the rest as the monthly returns have a wide dispersion.

Density distributions are useful to have an idea about the overall risk and return of assets as they summarize 48 observations (in this case 12 observations per year, $12 \times 4$). Some may argue 48 observations are not enough to conduct a financial analysis, but we prefer to keep it simply for now.

Now, let's propose some statistical indicators to complement our previous analysis.

```{r}
# Calculate relevant statistics.
FANG_stats <- FANG_monthly_returns %>%
summarise(mean = mean(monthly.returns), sd = sd(monthly.returns), 
          sr = mean/sd, iqr = IQR(monthly.returns))
FANG_stats
```

The mean is basically a measure of an expected return of the stock. The stock with the lowest mean is GOOG and the highest is NFLX. This is consistent with our previous graphical analysis. GOOG expected return is very close to zero, whereas NFLX expected return is definitely higher as the highest value corresponds to a positive return. The standard deviation (sd) is a measure of how disperse are the returns or in financial terms, how risky are the returns. The stock with less risk is GOOG and the stock with highest risk is NFLX. This is also consistent with our previous graphical analysis. IQR is also a measure of risk although we are not going to discuss this one right now.

Comparing return and risk, or mean and standard deviation of stock returns can be troublesome. This is because the relationship between risk and return is not perfectly proportional.

Let's see the risk and return in a plot.

```{r}
# Mean variance plot.
FANG_stats %>%
  ggplot(aes(x = sd, y = mean, color = symbol)) +
  geom_point(size = 5) +
  geom_text(aes(label = paste0(round(sr, 3))), 
            vjust = 2, color = "black", size = 3.5) +
  xlim(0.04, 0.18) + ylim(0.01, 0.06) +
  labs(title = "The higher the risk, the higher the return",
subtitle = "Numerical values represent return per unit of risk.",
       x = "Risk", y = "Return") + theme_tq()
```

According to our results, Netflix is the stock with the highest return and risk. Does this mean Netflix has the highest return per unit of risk? This is easy to find out by dividing the mean by the standard deviation. This indicator is called $sr$ in the table above. According to this, Netflix has the highest return per unit of risk compared with the rest of the stocks. This is clearly consistent with our previous analysis of cumulative returns.

The plot above measures returns in y-axis and risk in x-axes. In finance, we call this a mean-variance approach as mean is the measure of return and variance (or its square root the standard deviation) is the measure of risk. The mean-variance approach was a breakthrough in finance because we can clearly compare any kind of assets under the basis of risk and return.

Imagine stock 1 has a return of 5% and a risk of 4%, whereas stock 2 has a return of 6% and a risk of 8%. Although stock 2 has a higher return, this return does not compensate for the increase in the risk. In particular stock 1 has a 1.25 return per unit of risk (5/4), and stock 2 has a 0.75 return per unit of risk. In this case, we would prefer to invest our money in stock 1. Again, the relative values are useful to make decisions. Here the relative value is stock return per unit of risk.

Note how we can differentiate between a less risky and a riskier asset returns by looking at their correspondent distributions. Let's highlight these principles by using a brief example. Below we simulate a couple of asset returns by generating random values. Both artificially created assets have the same return (0%) and one is riskier than the other.

```{r}
# Create random assets.
less_risky <- data.frame(length = rnorm(10000, 0, 1))
more_risky <- data.frame(length = rnorm(10000, 0, 2))
# Name random assets.
less_risky$asset <- 'Less risky asset'
more_risky$asset <- 'Riskier asset'
assets <- rbind(less_risky, more_risky)
# Plot the assets.
ggplot(assets, aes(length, fill = asset)) + 
  geom_density(alpha = 0.5, adjust = 3)
```

The risky asset can take a wider range of returns. In particular, the risky asset can take values ranging from (about) −7% to +7%, whereas the less risky asset about −3% to +3%. In the code above we ask R to generate 10,000 random values following a normal distribution with mean zero (return), and a standard deviation of 1% and 2% for the less and riskier asset respectively.

# Asset pricing.

The aim of asset pricing theories is to determine the fundamental value of an asset. There is a close relation between this fundamental value and an appropriate return. Then, the main focus of asset pricing theories and models is to determine this appropriate return. Most models relate expected returns to risks investors have to bear and have to be compensated for. They differ mainly in the risk factors they allow to enter into the model.

Although a large number of risk factors have been proposed in the literature and many models are used, none of the presented models or any other models used by practitioners are able to explain the observed asset prices or returns sufficiently. While most models work quite well for time horizons of more than a year, they fail in explaining short term movements.

## What drives asset return changes?

What drives asset return changes? This is a big question in finance. Stock returns change as expectations of the stock market participants (buyers and sellers) change. Good expectations can be interpreted as an opportunity for investors to buy now looking to generate a profit in the future. Good expectations could increase the demand for this stock, the stock price increases and the return increases as well. Bad expectations could increase the supply for this stock as stock market participants are now willing to sell the stock before the stock price falls. An increase in the supply decreases the stock price and the return. In practice supply and demand of a given stock change at the same time and at different magnitudes most of the time, so it is not easy to explain and anticipate changes in returns. Note that we said "not easy", we are not saying "impossible".

Expectations are fueled by information, and information comes from data analysis in many forms including business news, macroeconomic news, new financial information of the firm available, market conditions, rumors, fake news, tweets, and many more. This data is subject to a wide variety of financial analysis and models, so it is not uncommon that one investor anticipates a price fall whereas others anticipate a price increase. Again, it is not easy to explain and anticipate changes in returns as measuring expectations seems like a complicated task. We can argue that changes in risk factors change expectations so risk factors drive changes in asset returns. Let's elaborate more on these risk factors.

Consider an extreme example about what we mean by explaining and anticipating changes in stock returns. Let's say someone has been awake for the last 24 hours and is driving after drinking too much alcohol. Here, we have two well known risk factors that can anticipate and explain a potential car accident. Anybody under any circumstances is exposed to a car accident, but here there are two very clear risk factors that explain and anticipate a potential car accident. Knowing the relevant and most important risk factors is important because we can teach young people how to avoid car accidents and if we do it very well, we could even decrease the rate of car accidents. There might be many risk factors but surely there are some more significant than others. Consider driving while listening to music at a high volume. I think it is not crazy to assume that driving drunk is more dangerous than driving while listening to music at a high volume. It sounds reasonable to assume that adding two risk factors like lack of sleep and alcohol increases the chances of a car accident significantly.

Asset pricing and financial econometrics are usually combined in order to find out the risk factors of stock return changes (among other things). A typical risk factor is firm size. In particular, there is some evidence that suggests that small firms have higher returns than big firms. The industry is also considered as a risk factor as there are some industries which exhibit consistently lower returns than others. There are also some financial ratios that can be interpreted as risk factors for stock returns. In sum, we have some understanding of the risk factors of stock returns but we still need to learn more about this topic.

Why are we interested to explain and anticipate changes in stock returns? Remember the value of stocks are directly related with firm's equity, and firm's equity with firm size and performance. In finance and economics, we are interested that firms can grow and perform well because they represent not only a supply for goods and services but they also represent new job opportunities. The population in most countries is increasing so we need more and better job opportunities. As long as more families can get more and better job conditions they could (if other conditions hold) increase their living standards. Firms are also a source of innovation, research and knowledge so they play a very important role in the economy. Therefore, as long as we understand what drives stock returns can help us to understand the evolution of the whole economy and this is closely related with the people's standards of living. At the end, we should care about the firm performance not because we are interested in having money machines, but because we should aim to transform this value increase into better living conditions for families.

Firms are not always good for the economy. Firms are run by individuals and individuals can be greedy sometimes. Firms can also make wrong decisions because they simply fail to conduct a correct analysis. Just before the US credit crunch 2007-2008 crisis, banks were interested in allocate sub-prime mortgages in order to transform them and sell them as "new" assets called asset back securities. This securitization process allowed banks to sell the credit risk to others in a not so transparent process and mechanism. Let me explain. Every mortgage represents an account payable for the bank because a family is expected to pay for that loan. Accounts payable are boring in the sense that you have to wait until you get paid, plus they are risky because families may default. By implementing a securitization, the bank can literally collect plenty of accounts payable or sub-prime mortgages and create one single new asset that can be sold. If you buy this asset you will have to pay the bank an amount of money today, in exchange you will get future payments for a number of years in the future. You might be interested in this deal because the future payments are attractive compared with the price of this new asset.

What happened then was that the borrowers default, sub-prime mortgages were not paid, asset back securities investors lost their money, and we fall into a global recession. This is relevant for us because credit rating agencies failed to assign a good estimate of the risk of asset back securities. In particular, they reported that these assets had a very high credit rating whereas in reality they were basically junk (sub-prime mortgages). In some cases, they reported the same credit rating as bonds, and this is just crazy. This missvaluation (or overvaluation) of the asset back securities motivated people to invest in these assets as they believed they were a bargain (high return and low risk assets), but in reality, they were simply high return and very high risky assets. This is why we need to understand what drives asset returns. And this is also why we argue that firms are not always good for the economy.

Let's come back to our main objective (drivers of stock returns) and apply a simple approach to start. We know that there are certain stocks that behave similarly. Consider Mastercard and Visa. Visa and MasterCard are the two largest payment processing networks in the world, they do not issue cards directly to the public, as do Discover and American Express, but rather through member financial institutions. Therefore, it is easy to understand that these two firms belong to the same industry, and they are exposed to very similar risk factors. In principle, if these firms are similar, their respective stock returns might behave similar as well. We may argue that the change of the returns of one can be explained by the change of the returns of the other.

Let's see how similar these two firms are by looking at their respective cumulative returns in a similar way as we did with FANG stocks. We first download the price data.

```{r}
# Download the price data.
stock_prices_MA_V <- c("MA", "V") %>%
    tq_get(get  = "stock.prices", from = "2015-01-01",
           to   = "2016-12-31") %>%
    group_by(symbol)
```

Then transform prices into cumulative returns and plot them.

```{r}
# Transform prices into log returns.
stock_cumret_MA_V <- stock_prices_MA_V %>%
    tq_transmute(adjusted,
                 periodReturn,
                 period = "daily",
                 type = "log",
                 col_rename = "returns") %>%
# Create the wealth index.
    mutate(wealth.index = 100 * cumprod(1 + returns))
# Visualize the wealth index.
ggplot(stock_cumret_MA_V, aes(x = date, y = wealth.index, 
                              color = symbol)) +
    geom_line(size = 1) + labs(title = "MA and V: wealth index",
    subtitle = "100 USD investment growth from 2015-01-01 to 2016-12-31.") +
    theme_tq() + scale_color_tq()
```

Apparently, these stock returns are indeed closely related. Ups and downs are very similar in this time-series. These are the exact values:

```{r}
# Last values of the wealth index.
stock_cumret_MA_V %>%
  select(-returns) %>%
  spread(symbol, wealth.index) %>%
  tail()
```

The 100 USD investment growth is basically the same in both assets.

Let's see the daily returns of both stocks in a scatter plot now. This allows us to confirm if both stock returns are related.

```{r}
# Show the return relationship.
stock_ret_MA_V <- stock_prices_MA_V %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "daily",
                 type       = "log",
                 col_rename = "returns") 

ggplot(stock_ret_MA_V, aes(x = date, y = returns, 
                           group = symbol)) +
  geom_line(aes(color = symbol), alpha = 0.6) +
  labs(title = "Visualizing relationship of stocks returns",
       subtitle = "Not very clear, right?") + theme_tq()
```

The line plot was not a good idea to show our main point. You cannot see anything clear here. Only ups and downs with no clear visual pattern. Let's propose a different way to illustrate the relationship of stock returns.

```{r}
# Show the return relationship.
stock_ret_MA_V  %>%
    spread(key = symbol, value = returns) %>%
    ggplot(aes(x = V, y = MA)) +
    geom_point(color = palette_light()[[1]], alpha = 0.5) +
    geom_smooth(method = "lm") +
    labs(title = "Visualizing relationship of stocks returns") + theme_tq()
```

Dropping the time and plotting coordinates looks better. The scatter plot above suggests that if Visa stock returns are low, Mastercard stock returns are low as well. By the same token, if Visa sock returns are high, Mastercard stock returns are high as well. We have 504 points; every point represents a pair of daily returns of both firms. Stock returns are then positively related, and this relationship seems to be strong and linear. We can add one line to the above plot to summarize this linear relationship between the 504 stock return pairs.

Note that this straight line summarizes quite well the relationship between the stock returns. This is, most of the points lie very close to the straight line. The general equation of a straight line is $y = a + bx$, the value $a$ is called intercept and $b$ is the line slope. The slope measures how steep the line is, the higher the slope the steeper the line. We care about the slope because it measures how related are $x$ and $y$. In particular, a slope of zero means that there is no relationship between $x$ and $y$, whereas a slope of 1 means that if $x$ increases 2, then we should expect $y$ increases by 2 as well. The slope is interesting because it is the magnitude of the relationship between $x$ and $y$. The sign of $b$ reveals whether the relationship is positive or negative. According to the above plot, we can anticipate that the slope $b$ is positive and it should be close to 1.

In this case the general model is $MA_i = \alpha+\beta V_i$, where $i=1,...,504$ as we have 504 observations of $MA$ and $V$. It is easy to estimate the values of $\alpha$ and $\beta$ by conducting a simple regression analysis. The regression analysis basically finds out the values of $\alpha$ and $\beta$ that fits better the observed relationship between $MA$ and $V$.

```{r}
# Prepare the data.
reg <- stock_ret_MA_V %>%
  spread(key = symbol, value = returns)
# Estimate the model.
lm(MA ~ V, data = reg) %>%
summary()
```

In general our model is $MA_i = \alpha+\beta V_i$, and in particular we have our model estimation result as $MA_i = 0.0001130+0.8133658 V_i$. Without going into details, our linear regression analysis suggests that a 1% return on $V$ is associated with a 0.8133658% return in $MA$. This is a good fit of the observations.

Let's evaluate one example.

```{r}
filter(reg, date=="2015-04-22")
```

Consider the observation pair $V=0.03989731$, $MA=0.03833513$. According to our regression line, the value of $MA$ at $V=0.03989731$ should be $MA = 0.0001130 + 0.03989731 \times 0.8133658 = 0.03256411$. The difference between the observed 3.833513% and the estimated 3.256411% is the estimation error. Note that the observed value is higher than the estimated value, this is the observation is slightly above the blue regression line.

Our results suggests that Visa represent a risk factor of Mastercard. This is because according to our model Mastercard return changes can be explained by changes in Visa returns. The relationship between this pair of stock returns according to beta is almost perfect as 0.8133658 is close to 1. We can also calculate the correlation of both stock returns. The correlation (or Pearson correlation) is a statistic that measures linear correlation between two variables. It has a value between +1 and −1. A value of +1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. The beta is similar but it could be higher than +1 or lower than −1.

```{r}
cor(reg$MA, reg$V)
```

In this case, the correlation of Visa and Mastercard stock returns is 0.8485187. This correlation value is very close to 1, so these variables are strongly linear correlated. This is important because we can argue that these two stocks are very similar, if one increases the other increases and if one decreases the other decreases. This behaviour makes sense because as we said before these firms are exposed to the same (or very similar) risk factors.

Let's visualize a 1:1 relationship just as a reference in the red line below.

```{r}
# Add a reference red line representing beta=1.
stock_ret_MA_V  %>%
    spread(key = symbol, value = returns) %>%
    ggplot(aes(x = V, y = MA)) +
    geom_point(color = palette_light()[[1]], alpha = 0.5) +
    geom_smooth(method = "lm") + 
    geom_abline(intercept = 0, color = "red") +
    labs(title = "Visualizing relationship of stocks returns") +
    theme_tq()
```

The strength of this fit can be measured by the R-squared. The R-squared value can be interpreted as the variation of $MA$ explained by variations of $V$. In particular, almost 72% of the variations of $MA$ is explained by variations in $V$. The rest (about 28%) remains unexplained according to the model. This 28% could be explained by adding more risk factors to the asset pricing model, or proposing an alternative estimation method.

Our example assumed that $MA$ can be explained by $V$. We have not introduced a formal model so we are free to relax this assumption and turn it around. What if we assume Visa depends on Mastercard? Note that the results are not so different.

```{r}
# Estimate the model.
lm(V ~ MA, data = reg) %>%
    summary()
```

Here, we estimate $V_i = \alpha+\beta MA_i$, and in particular we have $V_i = -1.104 \times 10^{6}+0.8852 MA_i$.

```{r PMF E2 q1}
# Create yearly returns.
FANG_daily_returns <- FANG %>%
    group_by(symbol) %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "daily",
                 type       = "arithmetic")
# Transform data.
library(tbl2xts)
f <- FANG_daily_returns %>% tbl_xts(., spread_by = "symbol")
# Correlation.
cor(f)
# Model.
lm(AMZN ~ FB + GOOG + NFLX, data = f) %>%
    summary()
```

## Single-index model.

The relationship of the risk-return trade-off is the heart of equilibrium asset pricing theories and models. The asset pricing theories are fascinating as they allow us to go deeper in the understanding of the determinants of asset returns. Here, we will skip many aspects regarding the theory, derivation, assumptions and comparison of asset pricing models, theories and estimation approaches. For a formal approach you should look for the references by the end of this document. Here, we will motivate the main ideas underlying the asset pricing models, the underlying programming approach, and what can we learn from them.

What if we propose that the return of an individual asset is partially explained by the market return? This proposal is similar as if we assume that the education level of one individual depends on the education level of the whole country. This proposal seems reasonable although not perfect. The market return is an aggregation of all individual asset returns traded in the stock market. There are several ways we can estimate the market return. One of them is to calculate the return of a stock index like the S&P500. The S&P500, or simply the S&P, is a stock market index that measures the stock performance of about 500 large companies listed on stock exchanges in the United States. It is one of the most commonly followed equity indexes, and many consider it to be one of the best representations of the US stock market.

Market indexes as the S&P500 are important for several reasons. We have discussed that stock price evolution is a good way to track the performance of a single company. However, we are usually interested in evaluating the performance of all the companies in the market, not only one or two. Stock market indexes are formed by several firms or in some cases many participants known as constituents. There is a wide variety of stock market indexes, some are country specific, or based on industries and other firm characteristics. By looking at the price or index evolution of these indexes we can track what happens with the firms in an aggregate approach. Then, an index value change in a day show what happened with the firms in average in that specific day. If the S&P500 increases in one day then we interpret this as if most of the firms that participate in the S&P500 increased its value in that particular day.

These are the S&P500 constituents ticker symbols, or in other words the 500 large companies that belongs to the S&P 500 index.

```{r}
# Load the package.
library(BatchGetSymbols)
sp500 <- GetSP500Stocks()
sp500$Tickers
```

These are the S&P500 constituents. Have you seen that there are more than 500? Companies listed in the index may have issued multiple types of common stock, this is why we do not have exactly 500.

Let's conduct a regression analysis as before. Now the model is $stock_{i,j} = \alpha_i + \beta_i SP500_j + \epsilon_i$, where sub-index $i$ represents a given stock, and $j$ the historical observations. Note that this model is very similar as the general equation of a straight line is $y = a + bx$. The model $stock_{i,j} = \alpha_i + \beta_i SP500_j + \epsilon_i$ is basically the single-index-model. It is called single because we assume there is only one risk factor which in this case is the market return. There are other multi-factor models that adds more factors (like factor *F* for example) and they look like this: $stock_{i,j} = \alpha_i + \beta_i SP500_j + \delta_i F_j + \epsilon_i$.

According to the single index model, the stock return is decomposed in three parts: a constant return $\alpha_i$, a component proportional to the market index $\beta_i SP500_j$, and a random and unpredictable component $\epsilon_i$. The intercept term is the expected value of the component of security $i$'s return that is independent of the market's performance. The beta coefficient is specific for each security and measures the security's sensitivity to the market. The random component represents the deviation of the return on the security from its expected value. The single-index model says that risks of individual securities arise from two sources: market or systematic risk，reflected in $\beta_i SP500_j$ and firm-specific risk，reflected in $\epsilon_i$. This simple dichotomy may oversimplify factors of real world uncertainty. For example，it ignores industry events，which affect many firms within a single industry but do not influence the macroeconomy as a whole.

Let's estimate the model. Here, we have selected 10 stocks, $i=1,...10$, and 72 monthly returns observations $j=1,...,72$. Our main estimation objective is to find $\alpha_i$ and $\beta_i$ for these 10 stocks.

First, we download the corresponding 10 stock returns.

```{r}
# Download individual asset returns.
R_stocks <- c("NEM", "AMCR", "CLX", "PEAK", "KR", "TXN", "F", "TXT", 
              "KLAC", "TEF") %>%
  tq_get(get  = "stock.prices", from = "2010-01-01", 
         to   = "2015-12-31") %>%
  group_by(symbol) %>% 
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "R_stocks")
R_stocks
```

Then, we download the S&P500 monthly returns, the symbol is \^GSPC.

```{r}
# Download the market index return.
R_market <- "^GSPC" %>%
  tq_get(get  = "stock.prices", from = "2010-01-01",
         to   = "2015-12-31") %>%
    tq_transmute(select     = adjusted, 
                 mutate_fun = periodReturn, 
                 period     = "monthly", 
                 col_rename = "R_market")
R_market
```

Now, we join them in the same variable or R object. This is convenient in the estimation procedure.

```{r}
# Prepare the database.
R_stocks_market <- left_join(R_stocks, R_market, by = c("date" = "date"))
R_stocks_market
```

We can use the *performance_fun* function to facilitate the regression analysis estimation.

```{r}
# 10 models estimated at once.
R_capm <- R_stocks_market %>%
  tq_performance(Ra = R_stocks, 
                 Rb = R_market, 
                 performance_fun = table.CAPM) %>%
select(symbol, Alpha, Beta, `R-squared`)
R_capm
```

Here, we have the results for the single index model estimation for the 10 stocks.

Please note how we arrange the stocks according to the beta. We have stocks $\beta$ from 0.15 to 1.74. We interpret this as the riskiness of the stock with respect to the market. The stock NEM has the lowest risk with respect to the market. In other words, stocks with low betas are less exposed to changes in the S&P500 this is why we argue that are not risky with respect to the market. Stocks with low betas might be exposed to other risk factors, but not the market. On the other hand, stocks with high betas like TEF are highly exposed to changes in the S&P500 this is why we argue that are risky with respect to the market. The beta of TEF is actually higher than 1, which means that TEF react more than proportional with respect to changes in the stock market.

We need a deeper econometric analysis to validate our interpretations. Here, we are not going to deal with a formal econometric interpretation but we can propose some arguments to better understand the stock behaviour. Alphas are all very close to zero. The R-squared is also relevant as it shows what proportion of changes in the stock returns are explained by changes in the stock market. Note that 57% of TXN stock return changes are explained by changes in the S&P500.

We can illustrate the previous results in a graphical way.

```{r}
R_stocks_market$symbol <- 
  factor(R_stocks_market$symbol, levels =
           unique(R_stocks_market$symbol))
# Plot all results.
R_stocks_market %>% 
  ggplot(aes(x = R_market, y = R_stocks, color = symbol)) + 
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE) + 
  facet_wrap(~symbol, ncol = 5) +
  geom_abline(intercept = 0, color = "black", linetype = 1) +
  theme_minimal() + 
  labs(x = "SP500 Return", y = "Asset Return", 
       title = "Relationship between asset return and market") +
  theme(legend.position = "none", legend.title = element_blank())
```

The stocks are sorted in such a way that the slope is increasing. I added a black line which represents a $\beta = 1$ to illustrate the cases in which the stock is less risky than the market and riskier than the market. The y-axis represents the asset returns and the x-axis the S&P500 returns.

We are interested in these results for a variety of reasons. Consider the following example. Imagine I am currently investing in the stock market and I anticipate a fall in the stock market. Some investors would prefer to sell their positions but imagine I need or want to stay for any reason. If so, then I might consider rearranging my portfolio investment to include more stocks with low betas (probably even negative betas). By doing so and if I was right with respect to the S&P500, then my position would not be severely affected because my portfolio is now more independent to the evolution of the stock market. On the other hand, if I anticipate given my analysis that the stock market will rise, then I could rearrange my portfolio to include more stocks with high betas (even higher than 1). By doing so and if I was right with respect to the S&P500, then my position would improve because my portfolio would generate more return, even more than the S&P500 itself.

In practice, we have to conduct a series of formal statistical tests to rely on our estimated betas. In particular, we are interested to have a statistically significant estimator among other things. Things might become complicated when you know that there are many ways in which we can estimate betas. Even if we take a different historical dataset length, we can get different betas. Is we take a different market benchmark we can get different betas. Financial sites usually report betas of the stock, and sometimes it is difficult to find out what was the process of estimating the model. Then, these kinds of estimations have to be done by a professional in the area. My recommendation is to study asset pricing theory and relevant financial econometric techniques to propose these kind of investment recommendations. Our previous interpretation assumes we have correctly estimated betas.

The Capital Asset Pricing Model (CAPM) was created by William Sharpe in 1964. He won the 1990 Nobel Prize in Economic Sciences, along with Harry Markowitz and Merton Miller, for developing models to assist with investment decision making like the CAPM. This model is similar to the single index model as the CAPM estimates the return of an asset based on the return of the market and the asset's linear relationship to the return of the market. This linear relationship is the stock's $\beta$ (beta) coefficient. The CAPM beta captures the linear relationship between the asset or portfolio and the market. This model is simple, but it can serve as a good base for the building of more complex models.

We can extend our analysis further. Given our 10 set of assets, we can calculate annualized returns and annualized Sharpe ratios (return per unit of risk).

```{r}
# Calculate annualized returns.
R_stocks_market %>%
    tq_performance(Ra = R_stocks, Rb = NULL, 
                   performance_fun = table.AnnualizedReturns) %>%
arrange(`AnnualizedSharpe(Rf=0%)`)
```

A stock beta is a measure of the individual asset return risk with respect to the market. The annualized Sharpe ratio above is a measure of the individual asset return per unit of risk. Let's visualize the previous table.

```{r}
# Calculate annualized returns.
R_stocks_market_stats <- R_stocks_market %>%
    tq_performance(Ra = R_stocks, Rb = NULL, 
                   performance_fun = table.AnnualizedReturns) %>%
# Mean variance plot.
  ggplot(aes(x = AnnualizedStdDev, y = AnnualizedReturn, color = symbol)) +
  geom_point(size = 5) +
  geom_abline(intercept = 0, color = "red") +
  geom_text(aes(label = paste0(round(`AnnualizedSharpe(Rf=0%)`, 3))), 
            vjust = 2, color = "black", size = 3.5) +
  geom_text(aes(label = paste0(symbol)), 
            vjust = -1, color = "black", size = 3.5) + 
  ylim(-.17, .35) +
  labs(title = "The higher the risk, the higher the return?",
subtitle = "Numerical values represent return per unit of risk.",
       x = "Risk", y = "Return") + theme_tq() +
  theme(legend.position = "none", legend.title = element_blank())
R_stocks_market_stats
```

I added a straight line of 45 degrees so any asset above the red line means a return per unit of risk above 1. By the same token, any asset below the red line means a return per unit of risk below 1.

```{r PMF E2 q3}
# Download individual asset returns.
R_stocks_mkt <- c("NEM", "AMCR", "CLX", "PEAK", "KR", "TXN", "F", "TXT", 
              "KLAC", "TEF", "^GSPC") %>%
  tq_get(get  = "stock.prices", from = "2010-01-01", 
         to   = "2015-12-31") %>%
  group_by(symbol) %>% 
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "r")
R_stocks_mkt

R_stocks_mkt %>%
    tq_performance(Ra = r, Rb = NULL, 
                   performance_fun = table.AnnualizedReturns) %>%
arrange(`AnnualizedSharpe(Rf=0%)`)


# Calculate annualized returns.
R_stocks_mkt_stats <- R_stocks_mkt %>%
    tq_performance(Ra = r, Rb = NULL, 
                   performance_fun = table.AnnualizedReturns) %>%
# Mean variance plot.
  ggplot(aes(x = AnnualizedStdDev, y = AnnualizedReturn, color = symbol)) +
  geom_point(size = 5) +
  geom_abline(intercept = 0, color = "red") +
  geom_text(aes(label = paste0(round(`AnnualizedSharpe(Rf=0%)`, 3))), 
            vjust = 2, color = "black", size = 3.5) +
  geom_text(aes(label = paste0(symbol)), 
            vjust = -1, color = "black", size = 3.5) + 
  ylim(-.17, .35) +
  labs(title = "The higher the risk, the higher the return?",
subtitle = "Numerical values represent return per unit of risk.",
       x = "Risk", y = "Return") + theme_tq() +
  theme(legend.position = "none", legend.title = element_blank())
R_stocks_mkt_stats

```

What if we use the values of betas and Sharpe ratios to form investment portfolios with these 10 assets? We call a portfolio to the new asset formed by several (smaller) investments in single assets. Let's start with a visualization of monthly return of a naive portfolio. A naive portfolio is basically formed by a 10% investment in each of the 10 assets. Here, we do not invest more in higher stock betas or in higher Sharpe ratio stocks. It is simply an equally weighted portfolio.

```{r}
# Weights.
wts <- c(0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)
# Portfolio creation.
portfolio_returns_monthly <- R_stocks_market %>%
    tq_portfolio(assets_col  = symbol, 
                 returns_col = R_stocks, 
                 weights     = wts, 
                 col_rename  = "Ra")
portfolio_returns_monthly %>%
  # Visualization.
    ggplot(aes(x = date, y = Ra)) +
    geom_bar(stat = "identity", fill = palette_light()[[1]]) +
    labs(title = "Portfolio monthly returns.",
         subtitle = "10% in each one of the 10 assets.",
caption = "Shows an above-zero trend meaning positive returns.",
         x = "", y = "Monthly Returns") +
    geom_smooth(method = "lm", color = "red") +
    theme_tq() + scale_color_tq() +
    scale_y_continuous(labels = scales::percent)
```

A monthly returns plot is a good representation, but we normally show the evolution of an investment's growth. In this case we consider an initial investment of 10,000 USD.

```{r}
# Cumulative returns.
portfolio_growth_monthly <- R_stocks_market %>%
    tq_portfolio(assets_col   = symbol, 
                 returns_col  = R_stocks, 
                 weights      = wts, 
                 col_rename   = "investment.growth",
                 wealth.index = TRUE) %>%
    mutate(investment.growth = investment.growth * 10000)
portfolio_growth_monthly %>%
    ggplot(aes(x = date, y = investment.growth)) +
    geom_line(size = 2, color = palette_light()[[1]]) +
    labs(title = "Portfolio growth of $10,000.",
         subtitle = "10% in each one of the 10 assets.",
         caption = "Now we can really visualize performance!",
         x = "", y = "Portfolio Value") +
    geom_smooth(method = "loess") +
    theme_tq() +
    scale_color_tq() +
    scale_y_continuous(labels = scales::dollar)
```

Looks OK. However, we are not sure if this is the best combination of assets. This is, what if the equally weighted portfolio is in fact the worst alternative? This is why we are interested in comparing this equally weighted portfolio with some other portfolios. In particular, a beta increasing portfolio and a Sharpe ratio increasing portfolio. These are basically incremental weights of 10%, 20%, 30% and 40% for the highest four betas and for the highest four Sharpe ratios.

```{r}
# Calculate annualized returns.
R_stocks_market %>%
  tq_performance(Ra = R_stocks, Rb = NULL, 
                   performance_fun = table.AnnualizedReturns) %>%
  arrange(`AnnualizedSharpe(Rf=0%)`) %>%
  left_join(R_capm,by = 'symbol')  %>%
  select(symbol, `AnnualizedSharpe(Rf=0%)`, Beta) 
```

```{r}
# Three portfolios.
weights <- c(
    0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1, # equally weighted
    0, 0, 0, 0, 0, 0, 0.1, 0.2, 0.3, 0.4, # sr increasing
    0, 0.2, 0.1, 0.3, 0, 0, 0.4, 0, 0, 0 # beta increasing
)

stocks <- c("NEM", "TEF", "F", "TXT", "AMCR", "PEAK", 
            "KLAC", "TXN", "CLX", "KR")
weights_table <-  tibble(stocks) %>%
    tq_repeat_df(n = 3) %>%
    bind_cols(tibble(weights)) %>%
    group_by(portfolio)
```

See the results.

```{r}
# See the evolution of three portfolios.
stock_returns_monthly_multi <- R_stocks_market %>%
    tq_repeat_df(n = 3)

portfolio_growth_monthly_multi <- stock_returns_monthly_multi %>%
    tq_portfolio(assets_col   = symbol, 
                 returns_col  = R_stocks, 
                 weights      = weights_table, 
                 col_rename   = "investment.growth",
                 wealth.index = TRUE) %>%
    mutate(investment.growth = investment.growth * 10000)
portfolio_growth_monthly_multi %>%
  ggplot(aes(x = date, y = investment.growth, color = factor(portfolio))) +
    geom_line(size = 2) +
    labs(title = "Portfolio growth of 10,000.",
         subtitle = "1: Equally weighted; 2: Sharpe ratio; 3: Beta",
         caption = "Portfolio 2 is a Standout!",
         x = "", y = "Portfolio Value",
         color = "Portfolio") +
    geom_smooth(method = "loess") +
    theme_tq() + scale_color_tq() +
    scale_y_continuous(labels = scales::dollar)
```

In this case the increasing Sharpe ratio portfolio is the best one. In particular, the portfolio is: 10% in KLAC, 20% in TXN, 30% in CLX, 40% in KR, and 0% in the rest.

## Predict asset prices.

In the previous section we were interested to explain what drives the changes of asset returns. By knowing these drivers, we could identify significant risk factors of individual assets. In the example above we propose that the risk factor was the market index (although there are more). We see that some assets are more related with the market than others. Up to this point, we know that NEM barely reacts to the changes in the S&P500 whereas TIF reacts more than proportional than the changes in the S&P500. The question that arises is, can we anticipate the evolution of the S&P500? Can we anticipate the changes in the risk factors? Because if we can, then we could use this information to anticipate the impact over individual assets. In fact, the kind of techniques and examples that we will explain in this section are as flexible that can be easily applied to any kind of asset prices.

In finance we care about the future so it makes sense to introduce some forecasting techniques in R. The forecasting techniques shown here do not depend on other external factors (as in the case of asset pricing models). You will realize that the only information we need to do these specific forecasts is the information of the past prices of the asset.

Let's download the data. In this case, the S&P500 index.

```{r}
# Download the data.
sp_500 <- "^GSPC" %>%
  tq_get(get  = "stock.prices", from = "1995-01-01",
         to   = "2017-12-31") %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = to.monthly,
               col_rename = "sp_500")
```

Now, let's see the data.

```{r}
# Split the data into training and test.
sp500_all <- ts(sp_500$sp_500, start = c(1995, 1), freq = 12)
sp500_training <- window(sp500_all, start = 1995, end = c(2014, 12))
sp500_test <- window(sp500_all, start = 2015)
# See the data before forecast.
plot(sp500_all, type = "l", main = "SP500 index evolution.
Train set in black, test set in red.")
lines(sp500_training, col = "black")
lines(sp500_test, col ="red")
abline(v = 2015, lty = 2, col = "red")
```

Most of us hear the weather forecast every morning and I think we agree it is not always accurate. In econometrics we care about this and we are interested in evaluating our forecasts. This is problematic because my current forecast in $t=0$ will be validated in $t=1$, but I need to know how good my forecast is in $t=0$, not in $t=1$. That is tricky. Since we do not have the Infinity Stone known as the Time Stone and we do not have Dr. Strange's skills, then we have to find an alternative solution. The solution is the following. Consider we assume that we have information until Dec 2014, so we are going to estimate our model up to Dec 2014 and forecast the following 36 months: 2015, 2016 and 2017. The train set is then from Jan 1995 to Dec 2014, and the test set from 2015 to 2017. The trick is that we actually know what happened in the test set, so we could be able to compare different forecasts and see which one was closer to what really happened. By doing this, we could have a better idea about how good is my today's forecast.

In statistics and econometrics, and in particular in time series analysis, an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model. Both of these models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting).

Without going into more details, we can estimate an ARIMA model to conduct our forecast.

```{r}
# Load the package.
library(forecast)
# Estimation of ARIMA model.
fit <- auto.arima(sp500_training)
# See results.
summary(fit)
```

This is the model estimation.

Now, let's see the forecast.

```{r}
# 36-month forecast.
for_sp500_all <- forecast(fit, h = 36)
# Evaluate percentage error.
cbind(arima_forecast = tail(for_sp500_all$mean, 36), 
      test_set = tail(sp500_test, 36), 
      percentage_error = 100*(tail(for_sp500_all$mean, 36) - 
      tail(sp500_test, 36))/(tail(for_sp500_all$mean, 36)))
```

There are months in which the forecast looks OK, but in others the difference is more than 10%. It's easier if we see the forecast.

```{r}
plot(for_sp500_all, main = "ARIMA forecast in blue.
Training set in black, test set in red.")
lines(sp500_test, col = "red")
abline(v = 2015, lty = 2, col = "red")
```

The ARIMA model overestimate the S&P500 at the first months, and then underestimate it. As a rule of thumb, long term forecasts are hard, they are usually not very accurate. This makes sense as it is easier to forecast tomorrow weather compared to 3 month forecast weather.

```{r}
library(fpp3)
sp500_all.tsibble <- as_tsibble(sp500_all)

sp500_training.tsibble <- as_tsibble(sp500_training)

sp500_training.tsibble %>%
model(AAN = ETS(sp500_training ~ error("A") + trend("A") + season("N"))) %>%
  forecast(h = 36) %>%
  autoplot(sp500_training.tsibble) +
  labs(title = "SP500",
       y = "Index") +
  guides(colour = guide_legend(title = "Forecast")) +
autolayer(sp500_all.tsibble)

```

```{r}
sp500_all <- ts(sp_500$sp_500, start = c(1995, 1), freq = 12)
sp500_all.tsibble <- as_tsibble(sp500_all)

sp500_training.tsibble %>%
model(hw = ETS(sp500_training ~ error("A") + trend("A") + season("A")))%>%
  forecast(h = 36) %>%
  autoplot(sp500_training.tsibble) +
  labs(title = "SP500",
       y = "Index") +
  guides(colour = guide_legend(title = "Forecast")) +
autolayer(sp500_all.tsibble)
```

```{r}
library(fpp3)
sp500_all <- ts(sp_500$sp_500, start = c(1995, 1), freq = 12)
sp500_all.tsibble <- as_tsibble(sp500_all)

sp500_all.tsibble %>%
  model(
    classical_decomposition(value, type = "additive")
  ) %>%
  components() %>%
  autoplot() +
  labs(title = "Classical additive decomposition of total
                  US retail employment")
```

We can compare ARIMA forecast with TBATS forecast. The TBATS model combines several components, making them a very good choice for forecasting. It constitutes the following elements:

-   T: Trigonometric terms for seasonality.
-   B: Box-Cox transformations for heterogeneity.
-   A: ARMA errors for short-term dynamics.
-   T: Trend.
-   S: Seasonal (including multiple and non-integer periods).

This is the implementation in R.

```{r}
# 36-month forecast.
for_tbats <- forecast::forecast(sp500_training, h = 36)
df_tbats = as.data.frame(for_tbats)
df_tbats <- ts(df_tbats$`Point Forecast`, start = c(2015, 1), freq = 12)
# Evaluate percentage error.
cbind(TBATS_forecast = df_tbats, 
      test_set=tail(sp500_test, 36), 
      percentage_error = 100*((df_tbats) - 
      tail(sp500_test, 36))/(df_tbats))
```

After 36 months (Dec 2017), ARIMA error is −16.33%, while TBATS is −11.6%. These might not be very impressive values. And in fact, we can do better by implementing more elaborated techniques, including machine learning. However, a 36 month forecast with a percentage error of −11.6% looks not so bad.

We can see all results together.

```{r}
# Plot all previous results.
plot(sp500_all, main = "ARIMA forecast in blue, TBATS in green.
Training set in black, test set in red.")
lines(sp500_training)
lines(sp500_test, col = "red")
lines(df_tbats, col = "green", lwd = 3)
lines(for_sp500_all$mean, col = "blue", lwd = 3)
abline(v = 2015, lty = 2, col = "red")
```

In finance we are interested in forecasting because these techniques allow us to have some certainty about the future. Since we take investment decisions today looking for a return in the future, we need to have some tools to anticipate the future the best way we can.

The story so far is the following. We care about firm performance, and the evolution of the stock return is a good indicator. We care about the determinants of stock returns and we propose that the market and other risk factors determine the stock returns. Then we care about the evolution of the market and risk factors as these anticipate changes in the stock returns. Forecasting techniques are only one alternative to have some idea about the evolution of asset prices, including a stock market index like the S&P500. We know that the S&P500 was 2,058.90 in December 2014, and our best forecast for Dec 2017 (36 months in the future), is 2,394.947 and in reality, it was 2,673.61 (a percentage error of −11.6%). This is how we can anticipate firm performance.

The next section deals with the actual investment problem. In which assets should we invest? How much should we invest in each asset? Those questions can be addressed by learning asset allocation or portfolio selection techniques.

# Asset allocation.

In finance, we are expected to take good financing and investment decisions. Asset allocation and portfolio theory are areas that show how we can take optimal investment decisions. In general, optimization is the act of achieving the best possible result under given circumstances. In our context, the best possible result is the highest return given a risk level, or alternatively the lowest risk given a return level. The "given circumstances" refer to a set of restrictions or constraints that we can face in the market. These restrictions are in the form of a maximum level of one asset in a portfolio, constraints about short sales, etc. Computers can solve complex optimization problems so it makes sense to use R and conduct asset allocation and tasks and estimate portfolio optimization models.

In principle, we all know that the popular wisdom suggests *not to put all the eggs in the same basket*. This suggestion makes sense as we should diversify as diversification is a way to manage the risk of losing all the eggs if something bad happens. For example, imagine I invest in two firms, one that sells ice cream and another that sells hot chocolate. Assume that as an exchange of my investment, I get some returns based on these firms' sales. My diversification in these two firms seems reasonable as the weather (relevant risk factor) affects the sales of these two companies in an opposite way. When it is hot, sales increases in the ice cream shop and decreases in the hot chocolate shop; and when it is cold, sales increases in the hot chocolate shop and decreases in the ice cream shop. Then, my revenues will be fairly constant every year, and this is desirable as I reduce the volatility of my revenues, they become less risky, the standard deviation of my revenues is small. You may remember our previous discussion about the correlation of Mastercard and Visa, the correlation of these firms' stock returns is 0.8480487 and we argue that this mean that they not only share the same risk factors, but they also react almost the same to changes in these risk factors, and this is why they behave similar. In the ice cream and hot chocolate shop example, we could guess that the sales' correlation of both shops have a strong negative correlation let's say −0.9. This means that the sales of both shops behave opposite and they are affected differently by the risk factor called weather.

If the popular wisdom suggests *not to put all the eggs in the same basket*, then, the interesting question is, how many eggs should we put in each basket? This is the equivalent to an investment recommendation in the form of how much to invest in each individual asset. We call portfolio to the new asset formed by several (smaller) investments in single assets. And we call a portfolio weight to the percentage invested in single assets. Asset allocation models allow us to estimate these optimal portfolio weights. Let's see an extreme example first. Imagine my current investment is 1% in the ice cream shop and 99% in the hot chocolate shop. This does not look like a good idea because these portfolio weights make the portfolio highly vulnerable to hot weather. After implementing an asset allocation optimization, the portfolio weights could be 60% of my money invested in the ice cream shop and 40% in the hot chocolate shop.

## The single-period problem.

The single-period problem is the simplest framework to implement asset allocation models. Here, we assume that the investor has some historical information about the assets in $t=0$ (today), she makes an investment decision today, and she expects a return in $t=1$, end of story. Let's see how we should distribute the 100% of available money to invest in four stocks using the *PortfolioAnalytics* R package.

We need data to start. For convenience, we use the monthly returns of the FANG database.

```{r}
FANG_monthly_returns <- FANG %>%
    group_by(symbol) %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "monthly",
                 type       = "arithmetic")
FANG_monthly_returns
```

The format of *FANG_monthly_returns* is not compatible with the *PortfolioAnalytics* package as the data is currently tidy. So, we have to transform the way our database looks. Originally, we have a column called "symbol" and we need each stock to have their corresponding name. We can implement this change easily.

```{r}
# Load the relevant packages.
library(PortfolioAnalytics)
library(ROI)
library(ROI.plugin.glpk)
library(ROI.plugin.quadprog)
library(dplyr)
library(tbl2xts)

fang <- 
  FANG_monthly_returns %>% tbl_xts(., spread_by = "symbol")
head(fang)
```

As you can confirm, the database *fang* is the same as we only changed the format. Following our discussion about the correlation, we calculate the correlation matrix for these 4 stocks.

```{r}
cor(fang)
```

The lowest correlation is between FB and AMZN (0.1846197). This means that both firms exhibit a weak linear relationship. On the other hand, we have GOOG and AMZN with a correlation of 0.6171376, which suggest a stronger linear relationship between these stock returns. In principle, the lower the correlation of our assets, the greater the diversification possibilities when forming a portfolio. Remember the ice cream and the hot chocolate shop example, we assumed a correlation of −0.9 and we were supposed to decrease the volatility of our sales. In practice, it is not very easy to find negative correlated assets. However, we can achieve diversification gains as long as the correlation value is less than +1.

Let's continue with our main objective. We are interested in an investment recommendation (how much to invest in each of the four assets). We first define a portfolio specification.

```{r}
# Create the portfolio specification
port_spec <- portfolio.spec(colnames(fang))
# Add a full investment constraint such that the weights sum to 1
port_spec <- add.constraint(portfolio = 
                              port_spec, type = "full_investment")
# Add a long only constraint such that the 
# weight of an asset is between 0 and 1
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")
# Add an objective to minimize portfolio standard deviation
port_spec <- add.objective(portfolio = port_spec,
                           type = "risk",
                           name = "StdDev")
# Add an objective to minimize portfolio standard deviation
port_spec <- add.objective(portfolio = port_spec,
                           type = "return",
                           name = "mean")
port_spec
```

The portfolio specification indicates that we have four assets available, these are: FB, AMZN, NFLX and GOOG. Constraints indicates that we are expected to spend 100% of the funds available (full investment). This is, the sum of the portfolio weights of the four assets should be 100%. The constraint long refers that individual portfolio weights have to be positive. The StdDev is an objective because we are interested in minimizing the standard deviation (risk), and we want to maximize the mean.

The alternative of the constraint *long_only* is to allow the model to suggest negative portfolio weights. Negative portfolio weights represent short sales. Short selling is an investment strategy that speculates on the decline in a stock. Sometimes short selling is not allowed so the *long_only* constraint will deliver only positive portfolio weights. The *optimize.portfolio* function allows us to solve our portfolio problem by taking a variety of methods, constraints and objectives. Let's run two of them (random and ROI) and then we can discuss their differences.

```{r}
# Solve the optimization problem.
set.seed(13)
opt_rand <- optimize.portfolio(fang, portfolio = port_spec,
                          optimize_method = "random", trace = TRUE)
opt_roi <- optimize.portfolio(fang, portfolio = port_spec, 
                              optimize_method = "ROI", trace = TRUE)
# Show optimization results.
opt_rand
opt_roi
```

The output above shows the portfolio weights, return and risk of the two optimal portfolios. Before showing the results of this optimization in a plot, let's look at the initial situation. In particular, the individual assets before adding any portfolio.

```{r}
ggplot(FANG_stats, aes(x = sd, y = mean, color = symbol)) +
  geom_point(size = 5) + geom_text(aes(label = paste0(symbol)), 
  vjust = 2, color = "black", size = 3.5) +
  xlim(0.05, 0.169) + ylim(0.01, 0.06) +
  labs(title = "The higher the risk, the higher the return",
       x = "Risk", y = "Return") + theme_tq()
```

```{r}
hist(rnorm(10000, 0.05919893, 0.16656346), 100, xlim = c(-0.6, 0.6),
     main = "NFLX")
sum(rnorm(10000, 0.05919893, 0.16656346) < -0.1)/10000
hist(rnorm(10000, 0.01757620, 0.05941703), 100, xlim = c(-0.6, 0.6),
     main = "GOOG") 
sum(rnorm(10000, 0.01757620, 0.05941703) < -0.1)/10000
```

Do you remember this plot above from previous sections? This is basically the four individual assets in the mean-variance space. The proposal of asset allocation is that instead of choosing one individual asset to invest in, we can do better by creating a portfolio (investing some specific quantities in each individual asset). As we argue before, the role of the optimization process is precisely to determine which allocation is the most efficient.

For example, I do not think you will be happy with a portfolio whose return is 0.035 (3.5% monthly return) and a risk of 0.15 (15% standard deviation). This is because you could do better than that. For example, Facebook has a similar return with a 10% risk. But what if I propose a portfolio with a return of 4% and a risk of 5%? That would be great compared with the individual assets alternatives as it is a 0.8 return per unit of risk.

Remember these are the return per unit of risk of the individual assets.

```{r}
# List of sr, individual assets.
FANG_monthly_sr <- FANG_stats %>%
  select(symbol, sr)
FANG_monthly_sr
```

The random technique (Random Portfolios Optimization) evaluates many investment recommendation alternatives, each one is one portfolio. In principle, the technique should recommend a portfolio which leads to a more attractive return per unit of risk alternative than investing in individual assets.

```{r}
# Plot results.
chart.RiskReward(opt_rand, risk.col = "StdDev", 
                 main = "Minimum Variance Optimization", 
                 xlim = c(0, 10),
                 return.col = "mean", chart.assets = TRUE)
legend("bottomright", legend = c("opt_rand", "equally_weighted"),
       col = c("blue", "orange"), pch = 19, cex = 0.8)
```

According to our portfolio specification and assets, gray circles represent available portfolios or feasible portfolios in the sense that they are possibilities given the individual assets. Note that there are some available portfolios with a lower risk than GOOG. In fact, the algorithm suggests the blue portfolio as the optimal portfolio. This blue portfolio is attractive because it has the same risk than GOOG, but it has higher expected return. Moreover, our optimal blue portfolio has a similar return as AMZN, but with less risk. So, apparently the blue alternative is an attractive investment recommendation. Note that the gray portfolios form a kind of frontier in this mean-variance plot. This frontier suggests that it is not possible to invest in a portfolio at the left of this frontier. The optimal portfolio lies just in the frontier.

See the blue optimal portfolio again. We would prefer any other portfolio located in the top (higher return) or at the left (lower risk), but that is impossible given the data. On the other hand, it is possible to achieve a portfolio located below (low return) or at the right (high risk), but that is not optimal. This is why optimal portfolios are those that lie in the frontier, those are the portfolios located in the extreme high and left of this mean-variance plot.

Then, what can we do to invest in the blue optimal portfolio?

```{r}
# Extract weight, risk and return.
opt_rand$weights
sum(opt_rand$weights)
opt_rand$opt_values
opt_rand$opt_values$mean / opt_rand$opt_values$StdDev
```

According to the opt_rand portfolio, we have to invest 29.8% in FB, 18.2% in AMZN, 0.5% in NFLX, and 47% in GOOG. The optimal portfolio has a 0.4488096 return per unit of risk. This is clearly a better alternative compared with investing in individual assets as we show below.

```{r}
# Add the new portfolio sr to the list.
FANG_monthly_sr <- 
  add_row(FANG_monthly_sr, symbol = "opt_rand", 
          sr = opt_rand$opt_values$mean / opt_rand$opt_values$StdDev) %>%
  arrange(sr)
FANG_monthly_sr
```

The yellow portfolio is also interesting as it represents an equally weighted portfolio. This is, 25% in each individual asset. We usually take this as a benchmark portfolio. You do not have to implement any optimization process to find out the 25% as it is basically $1/4$ or 1 over the number of assets. Interestingly, this equally weighted portfolio is close to being optimal but if you look closer, it is not optimal.

A second criterion is to optimize according to ROI (R Optimization Infrastructure for linear and quadratic programming solvers).

```{r}
# Plot results.
chart.RiskReward(opt_roi, risk.col = "StdDev", 
                 main = "Minimum Variance Optimization", 
                 xlim = c(0.05, 0.18),
                 return.col = "mean", rp = TRUE, chart.assets = TRUE)
points(0.05807126, 0.02606294, pch = 19, cex = 1.5, col = "black")
legend("bottomright", legend = c("ROI", "opt_rand"),
       col = c("blue", "black"), pch = 19, cex = 0.9)
```

A different optimization criterion leads to a different optimal portfolio. However, they are both optimal as they rely just on the efficient frontier.

```{r}
# Extract weight, risk and return.
opt_roi$weights
sum(opt_roi$weights)
opt_roi$opt_values
opt_roi$opt_values$mean / opt_roi$opt_values$StdDev
```

The ROI criterion recommends investing 38.3% in FB and 61.7% in NFLX. This led to a return per unit of risk of 0.4237292. Let's compare the alternatives.

```{r}
# Add the new portfolio sr to the list.
FANG_monthly_sr <- 
  add_row(FANG_monthly_sr, symbol = "opt_roi", 
          sr = opt_roi$opt_values$mean / opt_roi$opt_values$StdDev) %>%
arrange(sr)
FANG_monthly_sr
```

According to the return per unit of risk, we should choose the opt_rand portfolio because it has a 0.449 return per unit of risk. The optimization process succeeds at proposing a better investment strategy compared with the individual assets. In practice, if you were interested to form a diversified portfolio, you could implement an analysis like this one, with some more extensive tests, but very similar to this analysis. As an individual, you could contact your broker or your financial institution at $t=0$ to ask them to invest your money according to the opt_rand portfolio: 29.8% in FB, 18.2% in AMZN, 0.5% in NFLX, and 47% in GOOG, and you should expect to get the opt_rand portfolio risky return at $t=1$.

The expected return in $t=0$ of your opt_rand portfolio is 0.02606294. The value of 0.02606294 is simply the sum of the opt_rand portfolio weights multiplied by the individual asset returns. But, what will be your realized return at $t=1$? It is hard to wait until $t=1$ to know your return (or loss). When we introduced the forecasts methods, we used the training and test periods to evaluate what might happen in the future. We can do the same here, but I propose a different approach now. In particular, I propose to simulate the future.

In this case we invest in $t=0$ or 2016-12-30 and the future is $t=1$ or 2017-01-31. There are many ways to conduct the simulation. Let's keep it simple and use the historical mean and standard deviation information we have.

```{r}
FANG_stats_all <- FANG_stats %>%
  select(symbol,mean, sd) %>%
  add_row(symbol = "opt_rand", 
          mean = opt_rand$opt_values$mean, sd = opt_rand$opt_values$StdDev) %>%
  add_row(symbol = "opt_roi", 
          mean = opt_roi$opt_values$mean, sd = opt_roi$opt_values$StdDev)
FANG_stats_all
```

Let's simulate 1,000 observations of each individual asset and portfolios. We assume that the assets behave as a normal with mean and standard deviation as we show in the table above. This approach can be interpreted as if we were simulating 1,000 alternative values for 2017-01-31. By doing this, we could have a sense about what will be the most likely value at $t=1$.

```{r}
# Number of simulations.
sim = 1000
set.seed (7)
# Simulation per stock and portfolio.
s_AMZN <- rnorm(sim, 0.02566966, 0.08172605)
s_FB <- rnorm(sim, 0.03412852, 0.09894324)
s_GOOG <- rnorm(sim, 0.01757620, 0.05941703)
s_NFLX <- rnorm(sim, 0.05919893, 0.16656346)
s_ran <- rnorm(sim, opt_rand$opt_values$mean, opt_rand$opt_values$StdDev)
s_roi <- rnorm(sim, opt_roi$opt_values$mean, opt_roi$opt_values$StdDev)
```

The simulation is done. Now let's visualize the results in a boxplot. Boxplots show the distribution of the data in a data set. It divides the data set into three quartiles. This graph represents the minimum, maximum, median, first quartile and third quartile in the data set.

```{r}
# The boxplot.
b = cbind(s_FB, s_AMZN, s_NFLX, s_GOOG, s_ran, s_roi)
boxplot(b, las = 2, main = "Simulated returns of investment alternatives.")
abline(h = 0, lty = 2)
```

Note that the opt_rand portfolio is well diversified and opt_roi is more risky. This plot reveals a glimpse to the future assuming that the assets follow a normal distribution. This assumption is not as bad and it is useful to simplify the analysis. Are we satisfied with this glimpse to the future? Sometimes it is the best you can have at $t=0$. In this case, we fortunately know what really happened with these stocks on 2017-01-31. Let's evaluate the returns that really happened at $t=1$ for the individual and portfolios.

First, we need the 2017-01-31 actual or realized returns. We need to download the data as the FANG database ends at 2016-12-30.

```{r}
# Download the 2017-01-31 individual asset returns.
r_stocks <- c("FB", "AMZN", "NFLX", "GOOG") %>%
  tq_get(get  = "stock.prices", from = "2016-12-31", 
         to   = "2017-01-31") %>%
  group_by(symbol) %>% 
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               col_rename = "R_stocks") %>%
  tbl_xts(., spread_by = "symbol")
r_stocks
```

These are the realized monthly returns at $t=1$. Please note that we have not introduced these returns before. Our model and portfolios ignores these returns. Now, we need to evaluate the realized return of both optimal portfolios. This is done by adding the multiplication of weights and individual realized returns. This approach is commonly known as out-of-sample evaluation.

```{r}
# Calculate realized portfolio returns.
realized_ret <- c(rand = sum(opt_rand$weights*r_stocks),
                  roi = sum(opt_roi$weights*r_stocks))

expected_ret <- c(rand = opt_rand$opt_values$mean,
                  roi = opt_roi$opt_values$mean)
data.frame(cbind(realized_ret, expected_ret))
```

In sum, the expected monthly return in $t=0$ of opt_rand portfolio is 0.02606294, and the realized monthly return in $t=1$ is 0.06958923. Not bad. Let's see the whole thing now. This is, the distribution of the simulation and the realized returns in red.

```{r}
# The simulation.
boxplot(b, las = 2, main = "Simulated returns of investment alternatives.
What really happened by 2017-01-30 in red.")
abline(h=0, lty = 2)
# The realized returns in red.
points(1, 0.1208283,  col = "red", pch = 19, cex = 2)
points(2, 0.101782,   col = "red", pch = 19, cex = 2)
points(3, 0.1076947,  col = "red", pch = 19, cex = 2)
points(4, 0.02058157, col = "red", pch = 19, cex = 2)
points(5, sum(opt_rand$weights*r_stocks), col = "red", pch = 19, cex = 2)
points(6, sum(opt_roi$weights*r_stocks),  col = "red", pch = 19, cex = 2)
```

The realized returns can be quite different from the promised or expected returns. This difference depends on the method, the model, the database length, the ability of the portfolio designer, and also depends on a random component. In our FANG example, the investment recommendations were good alternatives.

It is tempting to calculate an optimal portfolio based on our previous CAPM example. Let's do it here just to show how easy is it to replicate the analysis with a different database, which is the beauty of reproducibility as I basically did a copy paste of the previous code.

```{r}
capm_stocks <- 
  R_stocks %>% tbl_xts(., spread_by = "symbol") %>%
  na.fill(fill = 0.00)
port_spec <- portfolio.spec(colnames(capm_stocks))
port_spec <- add.constraint(portfolio = 
                              port_spec, type = "full_investment")
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")
port_spec <- add.objective(portfolio = port_spec,
                           type = "risk",
                           name = "StdDev")
port_spec <- add.objective(portfolio = port_spec,
                           type = "return",
                           name = "mean")
set.seed(14)
opt_rand <- optimize.portfolio(capm_stocks, portfolio = port_spec,
                          optimize_method = "random", trace = TRUE)
chart.RiskReward(opt_rand, risk.col = "StdDev", 
                 main = "Minimum Variance Optimization",
                 return.col = "mean", chart.assets = TRUE)
legend("bottomleft", legend = c("opt_rand", "equally_weighted"),
       col = c("blue", "orange"), pch = 19, cex = 0.8)
abline(0, opt_rand$objective_measures$mean /
         opt_rand$objective_measures$StdDev, lwd = 2, col = "red")
```

Here, the slope of the red line corresponds to the optimal portfolio return per unit of risk. This means that the optimal portfolio has a similar return per unit of risk compared with KR. Note how the rest of the assets have a lower return per unit of risk.

```{r}
opt_roi <- optimize.portfolio(capm_stocks, portfolio = port_spec,
                          optimize_method = "ROI", trace = TRUE)
chart.RiskReward(opt_roi, risk.col = "StdDev", 
                 main = "Minimum Variance Optimization",
                 return.col = "mean", rp = TRUE, chart.assets = TRUE)
legend("bottomleft", legend = c("opt_roi"),
       col = c("blue"), pch = 19, cex = 0.8)
abline(0, opt_roi$objective_measures$mean /
         opt_roi$objective_measures$StdDev, lwd = 2, col = "red")

```

```{r PMF E2 q4, eval=FALSE, include=FALSE}
# Create yearly returns.
FANG_daily_returns <- FANG %>%
    group_by(symbol) %>%
    tq_transmute(select     = adjusted,
                 mutate_fun = periodReturn,
                 period     = "daily",
                 type       = "arithmetic")
library(tbl2xts)
f <- FANG_daily_returns %>% tbl_xts(., spread_by = "symbol")
cor(f)
set.seed(13)
opt_rand_daily <- optimize.portfolio(f, portfolio = port_spec,
                          optimize_method = "random", trace = TRUE)
opt_rand_daily
```

## Diversification.

In finance, diversification is the process of allocating capital (or creating a portfolio) in a way that reduces the exposure to any one particular asset or risk factor. We usually recommend to invest in a variety of assets to achieve an overall risk reduction of our portfolio. However, if those assets are as correlated as Visa and Mastercard (as discussed before), then our diversification efforts would not be so effective. In this section, we propose an experiment to illustrate the role of diversification in asset allocation. The experiment is to artificially generate two assets and add them to the FANG database. The special characteristic of these two new assets $X$ and $Y$ is that they both have a very extreme negative correlation value. Once we add these two assets, we will repeat the portfolio optimization and see if we could do better than before.

First, let's generate these two assets returns. Note that these two new assets $X$ and $Y$ do not exist in the real world, we are artificially generating them by implementing a simulation technique.

```{r}
# Library to use the multi-variate normal random number generator.
library('MASS')
set.seed(13)
data = mvrnorm(n = 48, mu = c(0.2, 0.5), 
               Sigma = matrix(c(1, -1.4, -1.4, 2), nrow = 2), 
               empirical = TRUE)/10
xy = as.data.frame(data)
X = xy$V1
Y = xy$V2
# Add X and Y to the fang database.
fang_xy <- fang
fang_xy$X <- X
fang_xy$Y <- Y
head(fang_xy)
```

Let's now verify that these two new assets are negatively correlated.

```{r}
cor(fang_xy)
```

The correlation of $X$ and $Y$ is −0.98994949, this is very close to −1, so they are strongly negatively correlated. This is just what we wanted. In fact, the correlation of these two new assets and the rest are also negative. In principle, we should expect a new optimal portfolio with a greater return per unit of risk.

Some may argue that investing in two inversely correlated assets would lead to a zero expected return. This is not the case as shown below.

```{r}
w_x <- seq(0, 1, 0.01)
w_y <- 1 - w_x
ret_xy <- w_x*mean(fang_xy$X) + w_y*mean(fang_xy$Y)
sd_xy <- (w_x^2*sd(fang_xy$X)^2 + w_y^2*sd(fang_xy$Y)^2 + 
  2*w_x*w_y*sd(fang_xy$X)*sd(fang_xy$Y)*cor(fang_xy$X, fang_xy$Y))^0.5
plot(sd_xy, ret_xy, type = "l", xlim = c(0, 0.15), ylim = c(0, 0.05))
abline(v = 0, lty = 2)
abline(h = 0, lty = 2)
points(sd(fang_xy$X), mean(fang_xy$X), cex = 2, pch = 19, col = "red")
points(sd(fang_xy$Y), mean(fang_xy$Y), cex = 2, pch = 19, col = "blue")
```

Let's generate the portfolio specification and optimize our portfolio as we did before.

```{r}
# Create the portfolio specification
port_spec <- portfolio.spec(colnames(fang_xy))
port_spec <- add.constraint(portfolio = 
                              port_spec, type = "full_investment")
port_spec <- add.constraint(portfolio = port_spec, type = "long_only")
port_spec <- add.objective(portfolio = port_spec,
                           type = "risk",
                           name = "StdDev")
port_spec <- add.objective(portfolio = port_spec,
                           type = "return",
                           name = "mean")
# Optimization.
set.seed(13)
opt_xy <- optimize.portfolio(fang_xy, portfolio = port_spec,
                          optimize_method = "random", trace = TRUE)
```

We are done. Now, let's visualize the results.

```{r}
# Plot results.
chart.RiskReward(opt_xy, risk.col = "StdDev", 
                 main = "Minimum Variance Optimization", xlim = c(0, 10),
                 return.col = "mean", chart.assets = TRUE)
points(0.05807126, 0.02606294, pch = 19, cex = 1.5, col = "black")
points(0.1170653,0.04960399, pch = 19, cex = 1.5, col = "red")
legend("topleft", legend = c("opt_xy (FANG+X+Y)", 
       "opt_rand (FANG)", "ROI (FANG)", "equally weighted (FANG)"),
       col = c("blue", "black", "red", "orange"), pch = 19, cex = 0.9)
```

This looks great as the new assets contribute to the optimization process to deliver a less risky portfolio. Now, the optimal portfolio has almost the same return as Facebook but with a significant reduced risk. Then, correlation value plays a determinant role in the diversification process. This is why investors are looking for low, or even better, negatively correlated assets.

Let's see the details of the new portfolio.

```{r}
# Extract weight, risk and return.
opt_xy$weights
opt_xy$opt_values
opt_xy$opt_values$mean / opt_xy$opt_values$StdDev
```

Note that the portfolio assigns high weights to $X$ and $Y$ with values of 52.2% and 43.4%. The rest is 0.2% in FB, 2.8% in AMZN, 0% in NFLX, and 1.4% in GOOG.

Let's compare the return per unit of risk.

```{r}
# Add the new portfolio sr to the list.
FANG_monthly_sr <- 
  add_row(FANG_monthly_sr, symbol = "opt_xy", 
          sr = opt_xy$opt_values$mean / opt_xy$opt_values$StdDev) %>%
  arrange(sr)
FANG_monthly_sr
```

Impressive improvement. Informed investors are not looking for high return assets to invest in. Specially not in the context of portfolio investment. High return assets are associated with high risk so it is likely not to get the promised (or expected) return. Let me put a silly example. You buy a lottery ticket for 2 USD and you expect to win 1,000,000 USD, however the odds to win are 1 in 12,607,306. A 999,998 USD return looks quite nice but you will hardly get it. This is why we argue that we do not pick an asset with respect to its price, nor with respect to its return, but with respect to its return per unit of risk. In the context of asset allocation (invest in several assets at a time), this is partially why informed investors are not looking for high return assets to invest in (although this sounds like the popular thought). Investors can do better by selecting low correlated assets because this will allow them to form a well diversified portfolio with a more certain return at $t=0$. In sum, contrary to popular wisdom, we can argue that in many circumstances we are more interested in risk rather than in return. In fact, this topic is called "risk management...", not "return management...".

If you are not quite happy with the 2.606294% monthly return of opt_rand portfolio, you always have the opt_roi which is 4.960399% and still with a decent return per unit of risk. If this is still not good for you, then (according to these models and assumptions) you should look to add an asset low or negatively correlated with your existing assets and conduct your optimization again. A good way to start looking at negative correlated assets is by looking at different and distant industries, or assets that belong to distant markets. This would at least improve the chances to find stocks with different responses to risk factors and this is a good way to start your search.

## Rebalancing portfolio and evaluation.

In the previous section we calculate portfolio weights once. We changed the optimization criteria and we added new assets, but we only calculate the portfolio weights once. Rebalancing is something very common in finance, it means to calculate portfolio weights as time passes. This makes sense because as time passes we have access to new information (stock returns) and we should re-balance our portfolio in order to take into account this new information. Evaluation is also a very common task in finance. We are interested to know what is the annualized return of an investment strategy in a period of time. In this section we are going to extend the asset allocation problem to incorporate rebalancing portfolio and evaluation. We are going to use a different database to illustrate our results.

Consider the following investment process. At $t=0$ I have access to 60 months historical information of a set of individual assets. Then, I can take information from $t=-60$ to $t=0$ to estimate optimal portfolio weights to form my portfolio at $t=0$. At $t=0$, $t=1$, and $t=2$ I simply get my returns or losses depending on the evolution of the market. Then, at $t=3$ I calculate new portfolio weights with information from $t=-57$ to $t=3$. At $t=3$, $t=4$, and $t=5$ I simply get my returns or losses depending on the evolution of the market. Then, at $t=6$ I calculate new portfolio weights with information from $t=-54$ to $t=6$. And I continue with the same procedure for several years. What would be my annualized return, and my annualized return per unit of risk? How could I know whether my investment procedure is better than other alternatives?

Before answering these questions, it is convenient to think in the process above. This looks like a lot of work. Everything starts with getting the price data for the correspondent assets. Then, convert the prices to returns. Then, calculate an optimal portfolio and implement the investment recommendation. Wait for the returns, and then re-balance our portfolio and implement the investment recommendation. Wait for the returns, and do the same until the end of the investment period which could last years. After that, look back to the portfolio returns and calculate an annualized return to evaluate my investment. This process is painful without a computer and without access to a computer language like R. In a computer we can automate this process and spend our time in more strategic tasks. Automatization is very common in other industries. Have you seen how cars are manufactured nowadays? You can hardly see a human operator, most of the process is made by robots. In finance, we can design robots since our main input (or raw material) is free data. Also, most of our main technology is free (R), the most expensive input is human capital.

Let's start with the data to tackle our objectives.

```{r}
# Get the data.
data(indexes)
returns <- indexes[, 1:4]
tail(returns)
```

The database goes from 1980-01-31 to 2009-12-31. The set of assets are: "US Bonds", "US Equities", "Int'l Equities" and "Commodities". This means that the first investment recommendation is calculated with information from 1980-01-31 to 1985-01-31 (60 monthly observations), the second from 1980-05-31 to 1985-09-31, and so on until the last period that goes from 2009-09-31 to 2009-12-31. The first monthly return will be the one on 1985-01-31 and the last on 2009-12-31, these are 300 monthly portfolio returns.

Before calculating optimal portfolios, let's calculate a benchmark portfolio. This will allow us to compare our optimal portfolio with a benchmark. In this case the benchmark is simply an equally weighted portfolio, investing 25% in each asset for all the periods. This equally weighted portfolio implies that we will not conduct any optimization. It is like investing 25% in each asset (or index in this case) and doing nothing until the end of the investment period.

```{r}
# Equal weight benchmark.
n <- ncol(returns)
equal_weights <- rep(1 / n, n)
benchmark_returns <- Return.portfolio(R = returns, 
                                      weights = equal_weights,
                                      rebalance_on = "quarters")
colnames(benchmark_returns) <- "benchmark"
# Benchmark performance.
table.AnnualizedReturns(benchmark_returns)
```

We are interested in a benchmark because it would facilitate our evaluation task.

Now we define the portfolio specification as we did in the section before.

```{r}
# Base portfolio specification.
base_port_spec <- portfolio.spec(assets = colnames(returns))
base_port_spec <- add.constraint(portfolio = base_port_spec,
                                 type = "full_investment")
base_port_spec <- add.constraint(portfolio = base_port_spec,
                                 type = "long_only")
base_port_spec <- add.objective(portfolio = base_port_spec, 
                                type = "risk", name = "StdDev")
```

We are ready to implement the investment process described before.

```{r}
# Run the optimization with periodic rebalancing.
opt_base <- optimize.portfolio.rebalancing(R = returns, 
            optimize_method = "ROI", portfolio = base_port_spec,
            rebalance_on = "quarters", training_period = 60, 
            rolling_window = 60)
# Calculate portfolio returns.
base_returns <- Return.portfolio(returns, extractWeights(opt_base))
colnames(base_returns) <- "base"
```

We are done. Rebalancing and evaluation are done. Now, let's show the results.

```{r}
# Chart the optimal weights.
chart.Weights(opt_base)
```

This is how re-balancing looks like. It is the contribution of each asset in my portfolio. Clearly US bonds dominate my portfolio but this is what the optimization recommends. Now, let's see the annualized performance of this portfolio.

```{r}
# Merge benchmark and portfolio returns.
ret <- cbind(benchmark_returns, base_returns)
# Annualized performance.
table.AnnualizedReturns(ret)
```

We did better than our benchmark portfolio, that is good.

Something that happens frequently is that for some reasons we face constraints about how much money to invest in an individual asset. Let's assume this is the case and that we are supposed not to invest more than 40% in US bonds. We can incorporate this constraint easily and reproduce the re-balancing chart and annualized returns.

```{r}
# Make a copy of the portfolio specification.
box_port_spec <- base_port_spec
# Update the constraint.
box_port_spec <- add.constraint(portfolio = box_port_spec,
                                type = "box", min = 0.05, max = 0.4,
                                indexnum = 2)
# Backtest.
opt_box <- optimize.portfolio.rebalancing(R = returns, 
                                          optimize_method = "ROI",
                                          portfolio = box_port_spec,
                                          rebalance_on = "quarters",
                                          training_period = 60,
                                          rolling_window = 60)
# Calculate portfolio returns.
box_returns <- Return.portfolio(returns, extractWeights(opt_box))
colnames(box_returns) <- "box"
```

In principle, more constraints will lead to a worse solution. The optimization algorithm works better as long as it can freely choose portfolio weights.

```{r}
# Chart the optimal weights.
chart.Weights(opt_box)
```

Now, the rest of the assets have a more significant role in our portfolio.

```{r}
# Merge box portfolio returns.
ret <- cbind(ret, box_returns)
# Annualized performance.
table.AnnualizedReturns(ret)
```

As expected, more constraints negatively impact the possibilities to get a higher annualized return.

Most of these activities including providing investment recommendations in a regular manner are services offered by private firms. People looking to invest part of their money are supposed to receive assistance and guidance and many times it is in the form of an investment recommendation like the ones we calculate here. In the past, these services were provided only by those firms with access to the knowledge, capital, experience, and technology. Nowadays, there are an increasing number of fintechs that provide financial services because now we have access to powerful software and improved computational capabilities. In any case, functions like \texttt{optimize.portfolio} are expected to be used as a tool by professionals and not as a pure source of investment recommendation.

```{r}
charts.PerformanceSummary(R = ret)

```

# Miscellaneous.

Here we present some others interesting examples.

## Multilevel modeling - data analysis.

The objective of this example is to show the power of data visualization, and introduce the concept of multilevel regression given the \texttt{hsb} database.

```{r include=FALSE}
# Load the relevant \texttt{R} packages.
library(lme4) # Linear Mixed-Effects Models.
library(lmerTest) # Tests in Linear Mixed Effects Models.
library(tidyverse) # Collection of packages.
library(merTools) # The database used in RISIS course is here.
library(sjPlot) # Plot lmer and glmer mods
library(MuMIn)
library(knitr) # To show formatted tables (kable).
```

The following is the description of the data taken from the \texttt{merTools} R package.

This is a subset of data from the 1982 High School and Beyond survey used as examples for HLM software. The data file used for this presentation is a subsample from the 1982 High School and Beyond Survey and is used extensively in Hierarchical Linear Models by Raudenbush and Bryk. It consists of 8 variables, and 7,185 students nested in 160 schools.

-   \texttt{schid}. A numeric vector, 160 unique values.
-   \texttt{mathach}. A numeric vector for the performance on a standardized math assessment.
-   \texttt{female}. A numeric vector coded 0 for male and 1 for female.
-   \texttt{ses}. A numeric measure of student socio-economic status.
-   \texttt{minority}. A numeric vector coded 0 for white and 1 for non-white students.
-   \texttt{schtype}. A numeric vector coded 0 for public and 1 for private schools.
-   \texttt{meanses}. A numeric, the average SES for each school in the data set.
-   \texttt{size}. A numeric for the number of students in the school.

Reference: Stephen W. Raudenbush and Anthony S. Bryk (2002). *Hierarchical Linear Models: Applications and Data Analysis Methods (2nd ed.)*. SAGE.

Let's see the data.

```{r}
# Let's take a look of the data and its original structure.
data(hsb) # Load the database.
head(hsb) # Look at the first values for inspection.
```

```{r}
#Now, let's inspect the data structure.
str(hsb) # See the original data structure.
```

Some important definitions.

```{r}
#This structure might not be completely appropriate as factors are not clearly  defined. Let's change that.
# From numerical to factors. This is useful for further analysis.
hsb$schid <- as.factor(hsb$schid)
hsb$minority <- as.factor(hsb$minority)
hsb$female <- as.factor(hsb$female)
hsb$schtype <- as.factor(hsb$schtype)
str(hsb) # See the new structure.
```

There is a weak, positive and linear relationship between the performance on a standardized math assessment and the socio-economic status of students. This is supported by a very basic and preliminar data analysis taking all information available, without any filter.

```{r}
ggplot(hsb, aes(x = ses, y = mathach)) + geom_point(alpha = 0.2) +
  geom_smooth(method = 'lm') +
  labs(title = "The big picture.", 
subtitle = "The higher the socio-economic status, the higher the 
math archievment.")
```

We propose that the student and the school characteristics matters in the performance of the math assessment. We have 160 schools, let's see only a sample of 10 and see how this linear relationship changes.

```{r}
set.seed(13)
s = sample(hsb$schid, 10)
ggplot(hsb[hsb$schid == s, ], aes(x = ses, y = mathach, group = schid,
                    color = schid)) + geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm', se = F) +
  labs(title = "School and student socio-economic status matters.", 
       subtitle = "A sample of 10 randomly selected schools.")
```

Now, the previous general trend is less clear as we see what happen in 10 randomly selected schools. This suggest that the relationship between socio-economic status of students and the math achievement depends (at least) on the school characteristics.

The performance on a standardized math assessment can be driven by the interaction of different factors. Here we see the differences between male and female students. According to this, males perform better than female students.

```{r}
ggplot(hsb, aes(mathach, fill = female)) + 
  geom_density(alpha = 0.5, adjust = 3) +
  labs(title = "Males versus females", 
       subtitle = "Males perform better than females in math assessments.")
```

The data analysis reveals a gender issue here. Then, the math achievement seems to be a multi-factor issue that depends on the school characteristics and the student gender. We can go one step back to reveal the role of gender in our original scatterplot.

```{r}
ggplot(hsb, aes(x = ses, y = mathach, 
                color = female)) + geom_point(alpha = 0.1) +
  geom_smooth(method = 'lm') +
  labs(title = "Males versus females by socio-economic status.", 
  subtitle = "Males perform better than females in math, and this
  difference slightly reduces as socio-economic status improves.")
```

This gender difference remains more or less constant across different socio-economic status. One may argue that this gap reduces as socio-economic status improves, but this reduction is minor, and the gap never closes.

One school characteristic is whether it is private or public. Here, we illustrate that private schools show a better performance on the standardized math assessments.

```{r}
ggplot(hsb, aes(mathach, fill = schtype)) + 
  geom_density(alpha = 0.5, adjust = 3) +
  labs(title = "Public versus private schools", 
       subtitle = "Private school students perform better than those 
       in public schools at math assessments.")
```

Although better math performance is achieved in private schools, the socio-economic status of the student can compensate in a way this effect. So, here, socio-economic status play an important role at making these school differences non significant.

```{r}
ggplot(hsb, aes(x = ses, y = mathach, 
                    color = schtype)) + geom_point(alpha = 0.1) +
  geom_smooth(method = 'lm') +
  labs(title = "Public versus private schools by socio-econonomic
       status", 
       subtitle = "The gap between public and private schools reduces 
       as the socio-economic status of students improves.")
```

Let's see what happens with minorities. Apparently, white students show a better performance on the standardized math assessments.

```{r}
ggplot(hsb, aes(mathach, fill = minority)) + 
  geom_density(alpha = 0.5, adjust = 3) +
  labs(title = "White versus non-white students", 
       subtitle = "White students perform better than non-white at 
       math assessments.")
```

Now, following the logic of our previous data analysis, we show the role of the socio-economic status in this minority gap.

```{r}
ggplot(hsb, aes(x = ses, y = mathach, 
                    color = minority)) + geom_point(alpha = 0.1) +
  geom_smooth(method = 'lm') +
  labs(title = "White versus non-white students.", 
  subtitle = "The gap tend to increase as socio-economic status
  improves.")
```

Minority differences increases as socio-economic status of the students improves. In other words, here the socio-economic status increases the gap between these two groups.

## Multilevel modeling - estimation.

We can learn a lot by visualizing the data. However, we can learn even more by implementing an econometric technique to help us understand the determinants of the math achievement.

*Two level multilevel regression. Null model: m1a.*

```{r}
# We start with the null model m1a.
m1a <- lmer(mathach ~ (1 | schid), data = hsb, REML = FALSE)
summary(m1a)
#plot(m1a, type=c("p","smooth"), col.line = 1)
m1a_AIC <- extractAIC(m1a)[2]
ans <- data.frame(m1a_AIC)
kable(t(ans), caption = "Model's AIC.", digits = 2)
```

*Two level multilevel regression. Second model: m2.*

```{r}
# Now, the model m2.
m2 <- lmer(mathach ~ ses + female + (1 | schid), data = hsb)
summary(m2)
# paste0("AIC = ",extractAIC(m2)[2])
m2_AIC <- extractAIC(m2)[2]
ans <- data.frame(m1a_AIC, m2_AIC)
kable(t(ans), caption = "Model's AIC.", digits = 2)

```

*Two level multilevel regression. Third model: m3.*

```{r}
# Now, the model called m3 in the course material.

m3 <- lmer(mathach ~ ses + female + size + meanses + schtype +
                 (1 | schid), data = hsb)
summary(m3)
#paste0("AIC = ",extractAIC(m3)[2])

m3_AIC <- extractAIC(m3)[2]
ans <- data.frame(m1a_AIC, m2_AIC, m3_AIC)
kable(t(ans), caption = "Model's AIC.", digits = 2)
```

*Two level multilevel regression. Optimize the model.*

```{r}
# We can propose an optimization of the model in terms of the AIC criterion.
options(na.action = "na.fail")
m_opt <- lmer(mathach ~ minority * female * ses  * schtype + 
                      (1 | schid), data = hsb, REML = FALSE)
mm <- dredge(m_opt)
head(model.sel(mm, rank = AIC))
```

*Two level multilevel regression. A good parsimonious model.*

```{r}
m_AIC <- lmer(mathach ~ minority + ses + minority*ses + 
                      (1 | schid), data = hsb, REML = FALSE)
summary(m_AIC)
#paste0("AIC = ",extractAIC(m_AIC)[2])

mpars_AIC <- extractAIC(m_AIC)[2]
ans <- data.frame(m1a_AIC, m2_AIC, m3_AIC, mpars_AIC)
kable(t(ans), caption = "Model's AIC.", digits = 2)

```

*Two level multilevel regression. A good not so parsimonious model.*

```{r}
m_AIC2 <- lmer(mathach ~  female + minority + schtype + 
                 ses + female*ses + minority*schtype + minority*ses +
                 schtype*ses +
                      (1 | schid), data = hsb, REML = FALSE)
summary(m_AIC2)
#paste0("AIC = ",extractAIC(m_AIC2)[2])

mnopars_AIC <- extractAIC(m_AIC2)[2]
ans <- data.frame(m1a_AIC, m2_AIC, m3_AIC, mpars_AIC, mnopars_AIC)
kable(t(ans), caption = "Model's AIC.", digits = 2)

```

## Multilevel modeling - final remark.

Conclusion. Data visualization help us to better understand the data relationships. These last two models have a low AIC compared with the models shown in the course.

```{r}
kable(t(ans), caption = "Model's AIC.", digits = 2)
```

# Conclusion.

Finance is not about how to make money; it is about much more than that. Finance is about how to find and use resources, and how to assign them into projects. This is not an easy task as resources are limited and projects are risky. Moreover, sometimes the financial markets do not work well: bad projects are financed and good projects are not. However, if we rely on finance theory, quantitative models, and data analysis, we increase the chances to make good and informed decisions. Making money is a necessary step that allows us to pursue superior objectives like making firms growth, create better jobs, stimulate innovation, economic growth, and hopefully improve the standard of living of the population.

We have made some progress at implementing financial and economic models to achieve these superior objectives but we cannot say we have succeeded. Poverty levels in some countries are high, and income inequality leads not only to economic but also social problems. Fortunately, there is an increasing and genuine interest in learning finance principles by good people around the world.

```{r include=FALSE}
a <- toc()
```

This document took `r as.numeric(a$toc-a$tic)` seconds to compile in Rmarkdown.

\newpage

# R and \LaTeX References.

The following are a constant growing list of online R and \LaTeX references.

## Instalation guides.

As you may understand, versions are frequently updated. If you find a newer video for installing a newer version please share. In any case, these videos can definitely help you as a guide to install the newer version available.

-   [Download & Install R 3.6.3](https://youtu.be/3OxPMYP8lNU)
-   [Download & Install RStudio Desktop 1.3.959](https://youtu.be/uvnuQ_fKrMc)
-   [Installing R and Rstudio on MacOS.](https://youtu.be/Y20P3u3c_1c)
-   [RStudio: A Guided Tour (by Jamison Crawford).](https://youtu.be/xgPwDlAtuNI)
-   [How to Install RStudio (and Knit to PDF).](https://youtu.be/2Sovzf6lVRo)
-   [RStudio: Explaining the Interface & R Markdown.](https://youtu.be/AHAR7j-IUOw)

## Main sites and program webpages.

-   [The R Project for Statistical Computing.](https://www.r-project.org/)
-   [RStudio.](https://www.rstudio.com/)
-   [DataCamp.](https://www.datacamp.com/)
-   [Tiny\TeX: A lightweight, cross-platform, portable, and easy-to-maintain \LaTeX distribution based on \TeX Live.](https://yihui.org/tinytex/)
-   [Swirl.](https://swirlstats.com/)
-   [Compile R online.](https://rextester.com/l/r_online_compiler)
-   [\LaTeX base.](https://latexbase.com)

## Books and e-books.

Learn R by reading these books.

-   [R For Dummies.](http://sgpwe.izt.uam.mx/files/users/uami/gma/R_for_dummies.pdf)
-   [R Programming for Data Science.](https://bookdown.org/rdpeng/rprogdatascience/)
-   [Foundations of Statistics with R.](https://mathstat.slu.edu/~speegle/_book/)
-   [Introduction to Econometrics with R.](https://www.econometrics-with-r.org/)
-   [Forecasting: Principles and Practice.](https://otexts.com/fpp3/)
-   [R Markdown Cookbook.](https://bookdown.org/yihui/rmarkdown-cookbook)
-   [R for Data Science.](https://r4ds.had.co.nz/)
-   [R Markdown: The Definitive Guide.](https://bookdown.org/yihui/rmarkdown/)
-   [Authoring Books and Technical Documents with R Markdown.](https://bookdown.org/yihui/bookdown)
-   [BOOKDOWN.](https://bookdown.org/)

## Blogs and tutorials.

Here you can find questions and answers about programming in R.

-   [R-Bloggers.](https://www.r-bloggers.com)
-   [Stack Overflow.](https://stackoverflow.com)
-   [R and Data Mining.](http://www.rdatamining.com)
-   [Revolutions.](https://blog.revolutionanalytics.com/)
-   [R-ladies.](https://rladies.org/)
-   [Interactive Tutorials for R.](https://rstudio.github.io/learnr/)
-   [Social Science Data and Statistics Resources.](https://researchguides.library.tufts.edu/data/r)
-   [Stack Exchange.](https://tex.stackexchange.com/)
-   [Kaggle.](https://www.kaggle.com/)
-   [Learn Data Science.](https://www.business-science.io/)

## Others.

-   [ProjectElon - Study Motivation.](https://www.youtube.com/c/ProjectElon/)
-   [iPanda.](https://www.youtube.com/c/iPandaChannel)
-   Twitter hashtags: \#rstats, \#DataScience
-   [Why Study a Postgraduate Degree in Quantitative Finance (by one of my current MSc students.)](https://youtu.be/7xpydgOrzkw)

## Books.

-   Elton EJ, MJ Gruber, SJ Brown and WN Goetzmann (2014) Modern Portfolio Theory and Investment Analysis, 9th Edition, New York: Wiley.
-   Hull JC (2017) Fundamentals of Futures and Options Markets, 8th Edition, Harlow UK: Pearson.
-   Hull JC (2019) Machine Learning in Business : An Introduction to the World of Data Science: Fourth Printing.
-   Richard A. Brealey, Stewart C. Myers, Franklin Allen. Principles of Corporate Finance, 10th Edition.
-   Crouhy M, D Galai and R Mark (2014) The Essentials of Risk Management, 2nd Edition, New York: McGraw Hill.
-   Matthews K and J Thompson (2014) The Economics of Banking. 3rd Edition. Chichester UK, John Wiley & Sons.
-   Chris Brooks (2014) Introductory Econometrics for Finance. Cambridge UK: Cambridge University Press.
-   Damodar Gujarati and Dawn Porter (2009) Basic Econometrics, Fifth edition, New York: McGraw-Hill.
-   Bodie, Z., Kane, A. and Marcus, A. (2011), Investments, 9th Edition, McGraw Hill Irwin.
